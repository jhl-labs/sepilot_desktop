# lib/domains/llm/ - LLM í´ë¼ì´ì–¸íŠ¸

> LLM(Large Language Model) API í†µì‹  ë° ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ë„ë©”ì¸

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [í´ë” êµ¬ì¡°](#í´ë”-êµ¬ì¡°)
- [ì£¼ìš” íŒŒì¼](#ì£¼ìš”-íŒŒì¼)
- [ì‚¬ìš© ë°©ë²•](#ì‚¬ìš©-ë°©ë²•)
- [Provider ì¶”ê°€ ê°€ì´ë“œ](#provider-ì¶”ê°€-ê°€ì´ë“œ)
- [ìŠ¤íŠ¸ë¦¬ë° íŒ¨í„´](#ìŠ¤íŠ¸ë¦¬ë°-íŒ¨í„´)
- [ì—ëŸ¬ ì²˜ë¦¬](#ì—ëŸ¬-ì²˜ë¦¬)
- [ì˜ˆì œ ì½”ë“œ](#ì˜ˆì œ-ì½”ë“œ)
- [ê´€ë ¨ ë¬¸ì„œ](#ê´€ë ¨-ë¬¸ì„œ)

---

## ê°œìš”

llm ë„ë©”ì¸ì€ SEPilot Desktopì˜ ëª¨ë“  LLM API í†µì‹ ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤. OpenAI, Anthropic, Google Gemini, Ollama ë“± ë‹¤ì–‘í•œ Providerë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™:**

- **Provider ì¶”ìƒí™”**: BaseLLMProvider ê¸°ë°˜ìœ¼ë¡œ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤
- **ìŠ¤íŠ¸ë¦¬ë° ìš°ì„ **: AsyncGenerator ê¸°ë°˜ ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¬ë°
- **ëŒ€í™”ë³„ ê²©ë¦¬**: conversationId ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì¤‘ ìŠ¤íŠ¸ë¦¼ ê´€ë¦¬
- **ì—ëŸ¬ ë³µì›ë ¥**: ì¬ì‹œë„, íƒ€ì„ì•„ì›ƒ, Fallback ì§€ì›

**ì§€ì› Provider:**

- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude 3.5, Claude 3)
- Google Gemini (Gemini Pro, Gemini Ultra)
- Ollama (ë¡œì»¬ LLM)
- OpenAI í˜¸í™˜ API (Groq, Together AI ë“±)

---

## í´ë” êµ¬ì¡°

```
lib/domains/llm/
â”œâ”€â”€ base.ts                   # BaseLLMProvider ì¶”ìƒ í´ë˜ìŠ¤
â”œâ”€â”€ client.ts                 # LLMClient ì‹±ê¸€í†¤
â”œâ”€â”€ service.ts                # LLMService (ê³ ìˆ˜ì¤€ API)
â”œâ”€â”€ providers/                # Provider êµ¬í˜„
â”‚   â”œâ”€â”€ openai.ts             # OpenAI ë° í˜¸í™˜ Provider
â”‚   â””â”€â”€ ollama.ts             # Ollama Provider
â”œâ”€â”€ streaming-callback.ts     # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ê´€ë¦¬
â”œâ”€â”€ vision-utils.ts           # ë¹„ì „ ëª¨ë¸ ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ http-utils.ts             # HTTP ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ web-client.ts             # ì›¹ í™˜ê²½ í´ë¼ì´ì–¸íŠ¸
â””â”€â”€ index.ts                  # Export
```

---

## ì£¼ìš” íŒŒì¼

### base.ts - BaseLLMProvider

**ì—­í• :** ëª¨ë“  LLM Providerì˜ ê³µí†µ ì¸í„°í˜ì´ìŠ¤ ì •ì˜

**ì£¼ìš” ë©”ì„œë“œ:**

```typescript
abstract class BaseLLMProvider {
  // ì¼ë°˜ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¬ë°)
  abstract chat(messages: Message[], options?: LLMOptions): Promise<string>;

  // ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
  abstract stream(messages: Message[], options?: LLMOptions): AsyncGenerator<string>;

  // ì„¤ì • ê²€ì¦
  abstract validate(config: LLMConfig): Promise<boolean>;

  // ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
  abstract getAvailableModels(): Promise<string[]>;
}
```

---

### client.ts - LLMClient

**ì—­í• :** LLM í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤, Provider ê´€ë¦¬ ë° ìŠ¤íŠ¸ë¦¬ë° ì œì–´

**ì£¼ìš” ê¸°ëŠ¥:**

- Provider ìë™ ì„ íƒ (ì„¤ì • ê¸°ë°˜)
- ìŠ¤íŠ¸ë¦¬ë° ìŠ¤ì¼€ì¤„ë§ (í•œ ë²ˆì— í•˜ë‚˜ì˜ ìŠ¤íŠ¸ë¦¼ë§Œ)
- ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ (AbortController)
- ëŒ€í™”ë³„ ì½œë°± ê²©ë¦¬

**ì‚¬ìš© ì˜ˆ:**

```typescript
import { LLMClient } from '@/lib/domains/llm/client';

const client = LLMClient.getInstance();

// ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
for await (const chunk of client.stream(messages, { conversationId: 'conv-123' })) {
  console.log(chunk);
}

// ì¼ë°˜ ì±„íŒ…
const response = await client.chat(messages);
```

**ì£¼ìš” ë©”ì„œë“œ:**

```typescript
class LLMClient {
  static getInstance(): LLMClient;

  // Provider ì´ˆê¸°í™”
  initialize(config: LLMConfig): void;

  // ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
  async *stream(messages: Message[], options?: LLMOptions): AsyncGenerator<string>;

  // ì¼ë°˜ ì±„íŒ…
  async chat(messages: Message[], options?: LLMOptions): Promise<string>;

  // ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨
  abort(conversationId: string): void;

  // í˜„ì¬ Provider
  getProvider(): BaseLLMProvider;
}
```

---

### service.ts - LLMService

**ì—­í• :** ê³ ìˆ˜ì¤€ LLM ì„œë¹„ìŠ¤, ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ ë° í”„ë¡¬í”„íŠ¸ ì „ì²˜ë¦¬

**ì£¼ìš” ê¸°ëŠ¥:**

- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì£¼ì…
- ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ê´€ë¦¬
- í† í° ì¹´ìš´íŒ…
- ë¹„ìš© ê³„ì‚°

---

### providers/openai.ts - OpenAIProvider

**ì—­í• :** OpenAI ë° í˜¸í™˜ Provider êµ¬í˜„

**ì§€ì› Provider:**

- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude 3.5, Claude 3)
- Google Gemini (Gemini Pro)
- Groq, Together AI, Perplexity ë“±

**ì£¼ìš” ê¸°ëŠ¥:**

```typescript
class OpenAIProvider extends BaseLLMProvider {
  async *stream(messages: Message[], options?: LLMOptions): AsyncGenerator<string> {
    const response = await fetch(this.endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${this.apiKey}`,
      },
      body: JSON.stringify({
        model: this.model,
        messages,
        stream: true,
        ...options,
      }),
    });

    const reader = response.body.getReader();
    const decoder = new TextDecoder();

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter((line) => line.trim() !== '');

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6);
          if (data === '[DONE]') return;

          try {
            const json = JSON.parse(data);
            const content = json.choices[0]?.delta?.content;
            if (content) yield content;
          } catch (e) {
            // íŒŒì‹± ì—ëŸ¬ ë¬´ì‹œ
          }
        }
      }
    }
  }
}
```

---

### providers/ollama.ts - OllamaProvider

**ì—­í• :** Ollama ë¡œì»¬ LLM Provider

**íŠ¹ì§•:**

- ë¡œì»¬ ì‹¤í–‰ (ì¸í„°ë„· ë¶ˆí•„ìš”)
- ì»¤ìŠ¤í…€ ëª¨ë¸ ì§€ì›
- ìŠ¤íŠ¸ë¦¬ë° API

**ì„¤ì • ì˜ˆ:**

```typescript
{
  provider: 'ollama',
  baseURL: 'http://localhost:11434',
  model: 'llama3.2',
}
```

---

### streaming-callback.ts - StreamingCallback

**ì—­í• :** ëŒ€í™”ë³„ ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ê²©ë¦¬ ë° ê´€ë¦¬

**ì£¼ìš” ê¸°ëŠ¥:**

```typescript
class StreamingCallbackManager {
  private callbacks = new Map<string, (chunk: string) => void>();

  // ì½œë°± ë“±ë¡
  register(conversationId: string, callback: (chunk: string) => void): void {
    this.callbacks.set(conversationId, callback);
  }

  // ì½œë°± í˜¸ì¶œ
  notify(conversationId: string, chunk: string): void {
    const callback = this.callbacks.get(conversationId);
    if (callback) callback(chunk);
  }

  // ì½œë°± ì œê±°
  unregister(conversationId: string): void {
    this.callbacks.delete(conversationId);
  }
}
```

**ì‚¬ìš© ì˜ˆ:**

```typescript
const manager = new StreamingCallbackManager();

manager.register('conv-123', (chunk) => {
  console.log('Conv 123:', chunk);
});

manager.register('conv-456', (chunk) => {
  console.log('Conv 456:', chunk);
});

// ê° ëŒ€í™”ë³„ë¡œ ê²©ë¦¬ëœ ì½œë°± ì‹¤í–‰
manager.notify('conv-123', 'Hello'); // "Conv 123: Hello"
manager.notify('conv-456', 'World'); // "Conv 456: World"
```

---

### vision-utils.ts - Vision Utils

**ì—­í• :** ë¹„ì „ ëª¨ë¸ ìœ í‹¸ë¦¬í‹° (ì´ë¯¸ì§€ ì…ë ¥ ì²˜ë¦¬)

**ì£¼ìš” ê¸°ëŠ¥:**

- ì´ë¯¸ì§€ â†’ Base64 ë³€í™˜
- ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì§• (í† í° ì ˆì•½)
- ë¹„ì „ ëª¨ë¸ ì§€ì› í™•ì¸

**ì‚¬ìš© ì˜ˆ:**

```typescript
import { prepareImageForVision, isVisionModel } from '@/lib/domains/llm/vision-utils';

if (isVisionModel('gpt-4-vision-preview')) {
  const base64 = await prepareImageForVision('/path/to/image.png');

  const messages = [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'ì´ ì´ë¯¸ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”' },
        { type: 'image_url', image_url: { url: `data:image/png;base64,${base64}` } },
      ],
    },
  ];
}
```

---

## ì‚¬ìš© ë°©ë²•

### 1. ê¸°ë³¸ ì‚¬ìš© (ìŠ¤íŠ¸ë¦¬ë°)

```typescript
import { LLMClient } from '@/lib/domains/llm/client';

const client = LLMClient.getInstance();

const messages = [
  { role: 'system', content: 'ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.' },
  { role: 'user', content: 'ì•ˆë…•í•˜ì„¸ìš”!' },
];

// ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
for await (const chunk of client.stream(messages, { conversationId: 'conv-123' })) {
  process.stdout.write(chunk);
}
```

### 2. ì¼ë°˜ ì±„íŒ… (ë¹„ìŠ¤íŠ¸ë¦¬ë°)

```typescript
const response = await client.chat(messages);
console.log('ì‘ë‹µ:', response);
```

### 3. ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨

```typescript
// Frontendì—ì„œ ì¤‘ë‹¨ ë²„íŠ¼ í´ë¦­
const handleAbort = () => {
  client.abort('conv-123');
};

// ë˜ëŠ” IPCë¥¼ í†µí•´
await window.electronAPI.llm.abort('conv-123');
```

### 4. ì´ë¯¸ì§€ í¬í•¨ (Vision ëª¨ë¸)

```typescript
import { prepareImageForVision } from '@/lib/domains/llm/vision-utils';

const imageBase64 = await prepareImageForVision('/path/to/image.png');

const messages = [
  {
    role: 'user',
    content: [
      { type: 'text', text: 'ì´ ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆë‚˜ìš”?' },
      {
        type: 'image_url',
        image_url: { url: `data:image/png;base64,${imageBase64}` },
      },
    ],
  },
];

for await (const chunk of client.stream(messages, { model: 'gpt-4-vision-preview' })) {
  console.log(chunk);
}
```

### 5. ì„¤ì • ë³€ê²½

```typescript
// Electron Main Process
import { LLMClient } from '@/lib/domains/llm/client';

const client = LLMClient.getInstance();

client.initialize({
  provider: 'anthropic',
  apiKey: 'sk-ant-...',
  model: 'claude-3-5-sonnet-20241022',
  maxTokens: 4096,
  temperature: 0.7,
});
```

---

## Provider ì¶”ê°€ ê°€ì´ë“œ

### 1. Provider í´ë˜ìŠ¤ ìƒì„±

**ì˜ˆì‹œ: HuggingFaceProvider**

```typescript
// lib/domains/llm/providers/huggingface.ts
import { BaseLLMProvider } from '../base';
import type { Message, LLMOptions, LLMConfig } from '@/types';

export class HuggingFaceProvider extends BaseLLMProvider {
  private apiKey: string;
  private model: string;
  private endpoint = 'https://api-inference.huggingface.co/models';

  constructor(config: LLMConfig) {
    super(config);
    this.apiKey = config.apiKey;
    this.model = config.model || 'mistralai/Mistral-7B-Instruct-v0.2';
  }

  async chat(messages: Message[], options?: LLMOptions): Promise<string> {
    const response = await fetch(`${this.endpoint}/${this.model}`, {
      method: 'POST',
      headers: {
        Authorization: `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        inputs: this.formatMessages(messages),
        parameters: {
          max_new_tokens: options?.maxTokens || 1024,
          temperature: options?.temperature || 0.7,
        },
      }),
    });

    const data = await response.json();
    return data[0]?.generated_text || '';
  }

  async *stream(messages: Message[], options?: LLMOptions): AsyncGenerator<string> {
    // HuggingFace ìŠ¤íŠ¸ë¦¬ë° API êµ¬í˜„
    // ...
    yield* this.streamResponse(messages, options);
  }

  async validate(config: LLMConfig): Promise<boolean> {
    try {
      const response = await fetch(`${this.endpoint}/${config.model}`, {
        headers: { Authorization: `Bearer ${config.apiKey}` },
      });
      return response.ok;
    } catch {
      return false;
    }
  }

  async getAvailableModels(): Promise<string[]> {
    return [
      'mistralai/Mistral-7B-Instruct-v0.2',
      'meta-llama/Llama-2-7b-chat-hf',
      'tiiuae/falcon-7b-instruct',
    ];
  }

  private formatMessages(messages: Message[]): string {
    // HuggingFace í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    return messages.map((m) => `${m.role}: ${m.content}`).join('\n');
  }
}
```

### 2. client.tsì— Provider ë“±ë¡

```typescript
// lib/domains/llm/client.ts
import { HuggingFaceProvider } from './providers/huggingface';

class LLMClient {
  initialize(config: LLMConfig): void {
    switch (config.provider) {
      case 'openai':
        this.provider = new OpenAIProvider(config);
        break;
      case 'ollama':
        this.provider = new OllamaProvider(config);
        break;
      case 'huggingface': // ì¶”ê°€
        this.provider = new HuggingFaceProvider(config);
        break;
      default:
        throw new Error(`Unknown provider: ${config.provider}`);
    }
  }
}
```

### 3. íƒ€ì… ì •ì˜ ì—…ë°ì´íŠ¸

```typescript
// types/index.d.ts
export type LLMProviderType = 'openai' | 'anthropic' | 'gemini' | 'ollama' | 'huggingface'; // ì¶”ê°€
```

### 4. UI ì„¤ì • ì¶”ê°€

```tsx
// components/settings/LLMSettings.tsx
<Select>
  <SelectItem value="openai">OpenAI</SelectItem>
  <SelectItem value="anthropic">Anthropic</SelectItem>
  <SelectItem value="ollama">Ollama</SelectItem>
  <SelectItem value="huggingface">HuggingFace</SelectItem>
</Select>
```

---

## ìŠ¤íŠ¸ë¦¬ë° íŒ¨í„´

### 1. AsyncGenerator ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°

**Provider êµ¬í˜„:**

```typescript
async *stream(messages: Message[], options?: LLMOptions): AsyncGenerator<string> {
  const response = await fetch(this.endpoint, {
    method: 'POST',
    body: JSON.stringify({ messages, stream: true }),
  });

  const reader = response.body!.getReader();
  const decoder = new TextDecoder();

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    const chunk = decoder.decode(value);
    // SSE íŒŒì‹±
    const lines = chunk.split('\n').filter(line => line.trim());

    for (const line of lines) {
      if (line.startsWith('data: ')) {
        const data = line.slice(6);
        if (data === '[DONE]') return;

        const json = JSON.parse(data);
        const content = json.choices[0]?.delta?.content;
        if (content) yield content;
      }
    }
  }
}
```

**ì‚¬ìš©:**

```typescript
for await (const chunk of provider.stream(messages)) {
  console.log(chunk);
}
```

### 2. ëŒ€í™”ë³„ ì½œë°± ê²©ë¦¬

**Electron IPCì—ì„œ ì‚¬ìš©:**

```typescript
// electron/ipc/handlers/llm/llm.ts
ipcMain.handle('llm-stream-chat', async (event, messages, options) => {
  const conversationId = options.conversationId;
  const client = LLMClient.getInstance();

  try {
    for await (const chunk of client.stream(messages, options)) {
      // conversationIdë¡œ ê²©ë¦¬ëœ ì´ë²¤íŠ¸ ì „ì†¡
      event.sender.send('llm-stream-chunk', {
        conversationId,
        chunk,
      });
    }

    event.sender.send('llm-stream-done', { conversationId });
  } catch (error) {
    event.sender.send('llm-stream-error', {
      conversationId,
      error: error.message,
    });
  }
});
```

### 3. Frontend ë¦¬ìŠ¤ë„ˆ

```tsx
// components/chat/unified/UnifiedChatArea.tsx
useEffect(() => {
  const handleChunk = (data: { conversationId: string; chunk: string }) => {
    if (data.conversationId === currentConversationId) {
      setContent((prev) => prev + data.chunk);
    }
  };

  window.electronAPI.on('llm-stream-chunk', handleChunk);

  return () => {
    window.electronAPI.off('llm-stream-chunk', handleChunk);
  };
}, [currentConversationId]);
```

---

## ì—ëŸ¬ ì²˜ë¦¬

### 1. ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬

```typescript
try {
  for await (const chunk of client.stream(messages)) {
    console.log(chunk);
  }
} catch (error) {
  if (error instanceof NetworkError) {
    console.error('ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì‹¤íŒ¨');
  } else if (error instanceof TimeoutError) {
    console.error('ìš”ì²­ ì‹œê°„ ì´ˆê³¼');
  }
}
```

### 2. API ì—ëŸ¬

```typescript
try {
  const response = await client.chat(messages);
} catch (error) {
  if (error.status === 401) {
    console.error('API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤');
  } else if (error.status === 429) {
    console.error('ìš”ì²­ í•œë„ ì´ˆê³¼');
  } else if (error.status === 500) {
    console.error('LLM ì„œë²„ ì—ëŸ¬');
  }
}
```

### 3. ì¬ì‹œë„ ë¡œì§

```typescript
async function retryStream(
  messages: Message[],
  options: LLMOptions,
  maxRetries = 3
): Promise<string> {
  let lastError: Error | null = null;

  for (let attempt = 0; attempt < maxRetries; attempt++) {
    try {
      let result = '';
      for await (const chunk of client.stream(messages, options)) {
        result += chunk;
      }
      return result;
    } catch (error) {
      lastError = error as Error;
      console.warn(`Retry ${attempt + 1}/${maxRetries}:`, error);

      // ì§€ìˆ˜ ë°±ì˜¤í”„
      await new Promise((resolve) => setTimeout(resolve, 1000 * Math.pow(2, attempt)));
    }
  }

  throw lastError;
}
```

---

## ì˜ˆì œ ì½”ë“œ

### ì˜ˆì œ 1: ê¸°ë³¸ ì±„íŒ…ë´‡

```typescript
import { LLMClient } from '@/lib/domains/llm/client';

const client = LLMClient.getInstance();
const conversationHistory: Message[] = [];

async function chat(userInput: string): Promise<string> {
  conversationHistory.push({ role: 'user', content: userInput });

  let assistantResponse = '';
  for await (const chunk of client.stream(conversationHistory, {
    conversationId: 'chatbot-123',
  })) {
    assistantResponse += chunk;
    process.stdout.write(chunk);
  }

  conversationHistory.push({ role: 'assistant', content: assistantResponse });
  return assistantResponse;
}

// ì‚¬ìš©
await chat('ì•ˆë…•í•˜ì„¸ìš”!');
await chat('ë‚ ì”¨ê°€ ì–´ë•Œìš”?');
```

### ì˜ˆì œ 2: ì´ë¯¸ì§€ í•´ì„

```typescript
import { LLMClient } from '@/lib/domains/llm/client';
import { prepareImageForVision } from '@/lib/domains/llm/vision-utils';

async function describeImage(imagePath: string): Promise<string> {
  const client = LLMClient.getInstance();
  const imageBase64 = await prepareImageForVision(imagePath);

  const messages = [
    {
      role: 'user',
      content: [
        { type: 'text', text: 'ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.' },
        {
          type: 'image_url',
          image_url: { url: `data:image/png;base64,${imageBase64}` },
        },
      ],
    },
  ];

  let description = '';
  for await (const chunk of client.stream(messages, {
    model: 'gpt-4-vision-preview',
  })) {
    description += chunk;
  }

  return description;
}
```

### ì˜ˆì œ 3: Function Calling

```typescript
import { LLMClient } from '@/lib/domains/llm/client';

const tools = [
  {
    type: 'function',
    function: {
      name: 'get_weather',
      description: 'íŠ¹ì • ë„ì‹œì˜ ë‚ ì”¨ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤',
      parameters: {
        type: 'object',
        properties: {
          city: { type: 'string', description: 'ë„ì‹œ ì´ë¦„' },
        },
        required: ['city'],
      },
    },
  },
];

async function chatWithTools(userInput: string) {
  const client = LLMClient.getInstance();

  const messages = [{ role: 'user', content: userInput }];

  for await (const chunk of client.stream(messages, {
    tools,
    tool_choice: 'auto',
  })) {
    // Tool call ì²˜ë¦¬
    if (chunk.includes('function_call')) {
      const functionCall = JSON.parse(chunk);
      const result = await executeFunction(functionCall.name, functionCall.arguments);

      messages.push({
        role: 'function',
        name: functionCall.name,
        content: JSON.stringify(result),
      });
    } else {
      console.log(chunk);
    }
  }
}
```

---

## ê´€ë ¨ ë¬¸ì„œ

### ë„ë©”ì¸

- [lib/README.md](../../README.md) - lib í´ë” ê°€ì´ë“œ
- [lib/domains/agent/README.md](../agent/README.md) - LangGraph Agent
- [lib/domains/mcp/README.md](../mcp/README.md) - MCP í†µí•©

### ì•„í‚¤í…ì²˜

- [docs/architecture/dependency-rules.md](../../../docs/architecture/dependency-rules.md) - ì˜ì¡´ì„± ê·œì¹™

### IPC í†µì‹ 

- [electron/ipc/README.md](../../../electron/ipc/README.md) - IPC í•¸ë“¤ëŸ¬ ê°€ì´ë“œ

### ê°œë°œ ê°€ì´ë“œ

- [CLAUDE.md](../../../CLAUDE.md) - í”„ë¡œì íŠ¸ ì „ì²´ ê°€ì´ë“œ

---

## ë³€ê²½ ì´ë ¥

- **2025-02-10**: Phase 3 ë¦¬íŒ©í† ë§ ì™„ë£Œ (ë„ë©”ì¸ êµ¬ì¡°í™”)
- **2025-01-17**: ì´ˆê¸° LLM í´ë¼ì´ì–¸íŠ¸ êµ¬ì¶•
