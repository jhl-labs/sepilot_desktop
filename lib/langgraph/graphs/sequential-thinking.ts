import { StateGraph, END } from '@langchain/langgraph';
import { ChatStateAnnotation, ChatState } from '../state';
import { LLMService } from '@/lib/llm/service';
import { Message } from '@/types';
import { createBaseSystemMessage } from '../utils/system-message';
import { emitStreamingChunk } from '@/lib/llm/streaming-callback';
import { getUserLanguage, getLanguageInstruction } from '../utils/language-utils';
import { createResearchNode } from '../utils/research-node';
import { logger } from '@/lib/utils/logger';
/**
 * Sequential Thinking Graph
 *
 * ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê·¸ë˜í”„
 * 1. ë¬¸ì œ ë¶„ì„ (Analyze)
 * 2. ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ (Plan)
 * 3. ê° ë‹¨ê³„ ì‹¤í–‰ (Execute)
 * 4. ìµœì¢… ë‹µë³€ ìƒì„± (Synthesize)
 */

/**
 * 0ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ (Research)
 */
const researchNode = createResearchNode<ChatState>('Sequential');

/**
 * 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„
 */
async function analyzeNode(state: ChatState) {
  logger.info('[Sequential] Step 1: Analyzing problem...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n## ğŸ” 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ë¬¸ì œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n\n', state.conversationId);

  // ìˆ˜ì§‘ëœ ì •ë³´(Research/RAG) ê°€ì ¸ì˜¤ê¸°
  const query = state.messages[state.messages.length - 1].content;
  const researchContext = state.context;

  if (researchContext) {
    emitStreamingChunk(`\nğŸ“š **ì‚¬ì „ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.**\n\n`, state.conversationId);
  }

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ëŠ” ì‚¬ë ¤ ê¹Šì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì„ íŒŒì•…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:
1. ì£¼ìš” ì§ˆë¬¸ ë˜ëŠ” ë¬¸ì œ
2. ê´€ë ¨ëœ í•µì‹¬ ê°œë…ë“¤
3. ë‹µë³€ì— í•„ìš”í•œ ì •ë³´

ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const analysisPrompt: Message = {
    id: 'analysis-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë¶„í•´í•˜ì„¸ìš”:\n\n${query}\n\n${researchContext ? `ìˆ˜ì§‘ëœ ì •ë³´:\n${researchContext}\n\n` : ''}ìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  // Skills ì£¼ì…
  const skillMessages: Message[] = [];
  try {
    const { skillsInjector } = await import('../skills-injector');
    const injectionResult = await skillsInjector.injectSkills(query, state.conversationId);

    if (injectionResult.injectedSkills.length > 0) {
      skillMessages.push(...skillsInjector.getMessagesFromResult(injectionResult));

      emitStreamingChunk(
        `\nğŸ¯ **${injectionResult.injectedSkills.length}ê°œì˜ Skillì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.**\n\n`,
        state.conversationId
      );

      logger.info('[Sequential] Skills injected:', {
        count: injectionResult.injectedSkills.length,
        skillIds: injectionResult.injectedSkills,
        tokens: injectionResult.totalTokens,
      });
    }
  } catch (skillError) {
    console.error('[Sequential] Skills injection error:', skillError);
    // Skill ì£¼ì… ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
  }

  let analysis = '';
  for await (const chunk of LLMService.streamChat(
    [systemMessage, ...skillMessages, ...state.messages, analysisPrompt],
    { tools: [], tool_choice: 'none' }
  )) {
    analysis += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  logger.info('[Sequential] Analysis complete:', `${analysis.substring(0, 100)}...`);

  return {
    context: `${researchContext ? `${researchContext}\n\n` : ''}# Analysis\n\n${analysis}`,
  };
}

/**
 * 2ë‹¨ê³„: ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½
 */
async function planNode(state: ChatState) {
  logger.info('[Sequential] Step 2: Planning solution steps...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## ğŸ“‹ 2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½ ì¤‘ì…ë‹ˆë‹¤...\n\n', state.conversationId);

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì „ëµì  ê³„íš AIì…ë‹ˆë‹¤. ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

í¬ê´„ì ì¸ ë‹µë³€ìœ¼ë¡œ ì´ì–´ì§ˆ ë‹¨ê³„ ëª©ë¡(3-5ë‹¨ê³„)ì„ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì‘ì„±í•˜ì„¸ìš”.
ê° ë‹¨ê³„ëŠ” ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const planPrompt: Message = {
    id: 'plan-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:\n\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}`,
    created_at: Date.now(),
  };

  let plan = '';
  for await (const chunk of LLMService.streamChat([systemMessage, planPrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    plan += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  logger.info('[Sequential] Plan complete:', `${plan.substring(0, 100)}...`);

  return {
    context: `${state.context}\n\n# Plan\n\n${plan}`,
  };
}

/**
 * 3ë‹¨ê³„: ë‹¨ê³„ë³„ ì‹¤í–‰
 */
async function executeNode(state: ChatState) {
  logger.info('[Sequential] Step 3: Executing plan...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## âš™ï¸ 3ë‹¨ê³„: ê³„íš ì‹¤í–‰\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ìˆ˜ë¦½ëœ ê³„íšì„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n', state.conversationId);

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ì‹ ì¤‘í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ì„¸ë¶€ ì§€í–¥ì ì¸ AIì…ë‹ˆë‹¤.

ê° ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ì„œ ìƒì„¸í•œ ì¶”ë¡ ê³¼ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
ì² ì €í•˜ê²Œ ì—¬ëŸ¬ ê°ë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const executePrompt: Message = {
    id: 'execute-prompt',
    role: 'user',
    content: `ì´ ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ìƒì„¸íˆ ì‹¤í–‰í•˜ì„¸ìš”:\n\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}`,
    created_at: Date.now(),
  };

  let execution = '';
  for await (const chunk of LLMService.streamChat([systemMessage, executePrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    execution += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  logger.info('[Sequential] Execution complete:', `${execution.substring(0, 100)}...`);

  return {
    context: `${state.context}\n\n# Execution\n\n${execution}`,
  };
}

/**
 * 4ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
 */
async function synthesizeNode(state: ChatState) {
  logger.info('[Sequential] Step 4: Synthesizing final answer...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## âœ¨ 4ë‹¨ê³„: ìµœì¢… ë‹µë³€\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ìµœì¢… ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: createBaseSystemMessage(),
    created_at: Date.now(),
  };

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  const synthesizePrompt: Message = {
    id: 'synthesize-prompt',
    role: 'user',
    content: `ìœ„ì˜ ëª¨ë“  ë¶„ì„, ê³„íš, ì‹¤í–‰ì„ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì ì¸ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

${state.context}

ì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}

ìœ„ ì‚¬ê³  ê³¼ì •ì˜ ëª¨ë“  í†µì°°ì„ í¬í•¨í•˜ëŠ” ëª…í™•í•˜ê³  ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  let finalAnswer = '';
  const messageId = `msg-${Date.now()}`;

  for await (const chunk of LLMService.streamChat([systemMessage, synthesizePrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    finalAnswer += chunk;
    // Send each chunk to renderer via callback for real-time streaming (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  // ì‚¬ê³  ê³¼ì • í¬ë§·íŒ…
  const processNodes = state.context
    .replace(/# Analysis/g, '## ğŸ” 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„')
    .replace(/# Plan/g, '## ğŸ“‹ 2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½')
    .replace(/# Execution/g, '## âš™ï¸ 3ë‹¨ê³„: ê³„íš ì‹¤í–‰');

  const finalContent = `${processNodes}\n\n---\n\n## âœ¨ ìµœì¢… ë‹µë³€\n\n${finalAnswer}`;

  const assistantMessage: Message = {
    id: messageId,
    role: 'assistant',
    content: finalContent,
    created_at: Date.now(),
  };

  logger.info('[Sequential] Final answer generated:', `${finalAnswer.substring(0, 100)}...`);

  return {
    messages: [assistantMessage],
  };
}

/**
 * Sequential Thinking Graph ìƒì„±
 */
export function createSequentialThinkingGraph() {
  const workflow = new StateGraph(ChatStateAnnotation)
    // ë…¸ë“œ ì¶”ê°€
    .addNode('research', researchNode)
    .addNode('analyze', analyzeNode)
    .addNode('plan', planNode)
    .addNode('execute', executeNode)
    .addNode('synthesize', synthesizeNode)
    // ìˆœì°¨ì  ì—£ì§€
    .addEdge('__start__', 'research')
    .addEdge('research', 'analyze')
    .addEdge('analyze', 'plan')
    .addEdge('plan', 'execute')
    .addEdge('execute', 'synthesize')
    .addEdge('synthesize', END);

  return workflow.compile();
}
