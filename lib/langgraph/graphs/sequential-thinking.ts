import { StateGraph, END } from '@langchain/langgraph';
import { ChatStateAnnotation, ChatState } from '../state';
import { LLMService } from '@/lib/llm/service';
import { Message } from '@/types';
import { createBaseSystemMessage } from '../utils/system-message';
import { emitStreamingChunk, getCurrentGraphConfig } from '@/lib/llm/streaming-callback';
import { generateWithToolsNode } from '../nodes/generate';
import { toolsNode } from '../nodes/tools';

/**
 * Sequential Thinking Graph
 *
 * ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê·¸ë˜í”„
 * 1. ë¬¸ì œ ë¶„ì„ (Analyze)
 * 2. ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ (Plan)
 * 3. ê° ë‹¨ê³„ ì‹¤í–‰ (Execute)
 * 4. ìµœì¢… ë‹µë³€ ìƒì„± (Synthesize)
 */

/**
 * RAG ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜
 */
async function retrieveContextIfEnabled(query: string): Promise<string> {
  const config = getCurrentGraphConfig();
  if (!config?.enableRAG) {
    return '';
  }

  try {
    // Main Process ì „ìš© ë¡œì§
    if (typeof window !== 'undefined') {
      return '';
    }

    console.log('[Sequential] RAG enabled, retrieving documents...');
    const { vectorDBService } = await import('../../../electron/services/vectordb');
    const { databaseService } = await import('../../../electron/services/database');
    const { initializeEmbedding, getEmbeddingProvider } =
      await import('@/lib/vectordb/embeddings/client');

    const configStr = databaseService.getSetting('app_config');
    if (!configStr) {
      return '';
    }
    const appConfig = JSON.parse(configStr);
    if (!appConfig.embedding) {
      return '';
    }

    initializeEmbedding(appConfig.embedding);
    const embedder = getEmbeddingProvider();
    const queryEmbedding = await embedder.embed(query);
    const results = await vectorDBService.searchByVector(queryEmbedding, 5);

    if (results.length > 0) {
      console.log(`[Sequential] Found ${results.length} documents`);
      return results.map((doc, i) => `[ì°¸ê³  ë¬¸ì„œ ${i + 1}]\n${doc.content}`).join('\n\n');
    }
  } catch (error) {
    console.error('[Sequential] RAG retrieval failed:', error);
  }
  return '';
}

/**
 * 0ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ (Research)
 */
async function researchNode(state: ChatState) {
  console.log('[Sequential] Step 0: Researching...');
  emitStreamingChunk('\n\n## ğŸ” 0ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ (Research)\n\n', state.conversationId);

  // RAG ê²€ìƒ‰
  const query = state.messages[state.messages.length - 1].content;
  const ragContext = await retrieveContextIfEnabled(query);

  let gatheredInfo = ragContext ? `[RAG ê²€ìƒ‰ ê²°ê³¼]\n${ragContext}\n\n` : '';

  // ë„êµ¬ ì‚¬ìš© ë£¨í”„ (ìµœëŒ€ 5íšŒ)
  let currentMessages = [...state.messages];

  // ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì •ë³´ ìˆ˜ì§‘ê°€ í˜ë¥´ì†Œë‚˜
  const researchSystemMsg: Message = {
    id: 'system-research',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì‹¬ì¸µ ë¶„ì„ì„ í•˜ê¸° ì „, í•„ìš”í•œ ë°°ê²½ ì§€ì‹ê³¼ ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì—°êµ¬ì›ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë„êµ¬(ê²€ìƒ‰ ë“±)ë¥¼ í™œìš©í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.
ì´ë¯¸ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆê±°ë‚˜ ë„êµ¬ê°€ ì—†ë‹¤ë©´ ì¦‰ì‹œ ì¢…ë£Œí•˜ì„¸ìš”.
ìµœëŒ€ 3íšŒì˜ ê¸°íšŒê°€ ìˆìŠµë‹ˆë‹¤.`,
    created_at: Date.now(),
  };

  currentMessages = [researchSystemMsg, ...currentMessages];

  for (let i = 0; i < 3; i++) {
    // Generate (ë„êµ¬ ì‚¬ìš© ê²°ì •)
    const genResult = await generateWithToolsNode({
      ...state,
      messages: currentMessages,
      toolCalls: [],
      toolResults: [],
    });
    const responseMsg = genResult.messages?.[0];

    if (!responseMsg) {
      break;
    }

    currentMessages.push(responseMsg);

    if (!responseMsg.tool_calls || responseMsg.tool_calls.length === 0) {
      break;
    }

    // Tools Execute
    const toolNames = responseMsg.tool_calls.map((tc) => tc.name).join(', ');
    emitStreamingChunk(`\nğŸ› ï¸ **ì •ë³´ ìˆ˜ì§‘ ì¤‘:** ${toolNames}...\n`, state.conversationId);

    const toolResult = await toolsNode({
      ...state,
      messages: currentMessages,
      toolCalls: [],
      toolResults: [],
    });

    // ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
    const toolMessages = (toolResult.toolResults || []).map((res) => ({
      role: 'tool' as const,
      tool_call_id: res.toolCallId,
      name: res.toolName,
      content: res.result || res.error || '',
      id: `tool-${res.toolCallId}`,
      created_at: Date.now(),
    }));

    currentMessages.push(...toolMessages);

    // ìˆ˜ì§‘ëœ ì •ë³´ ëˆ„ì 
    gatheredInfo += `[ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: ${toolNames}]\n${toolMessages.map((m) => m.content).join('\n')}\n\n`;

    emitStreamingChunk(`âœ… **ìˆ˜ì§‘ ì™„ë£Œ**\n`, state.conversationId);
  }

  console.log('[Sequential] Research complete');

  return {
    context: gatheredInfo,
  };
}

/**
 * 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„
 */
async function analyzeNode(state: ChatState) {
  console.log('[Sequential] Step 1: Analyzing problem...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n## ğŸ” 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ë¬¸ì œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n\n', state.conversationId);

  // ìˆ˜ì§‘ëœ ì •ë³´(Research/RAG) ê°€ì ¸ì˜¤ê¸°
  const query = state.messages[state.messages.length - 1].content;
  const researchContext = state.context;

  if (researchContext) {
    emitStreamingChunk(`\nğŸ“š **ì‚¬ì „ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.**\n\n`, state.conversationId);
  }

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ëŠ” ì‚¬ë ¤ ê¹Šì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì„ íŒŒì•…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:
1. ì£¼ìš” ì§ˆë¬¸ ë˜ëŠ” ë¬¸ì œ
2. ê´€ë ¨ëœ í•µì‹¬ ê°œë…ë“¤
3. ë‹µë³€ì— í•„ìš”í•œ ì •ë³´

ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const analysisPrompt: Message = {
    id: 'analysis-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë¶„í•´í•˜ì„¸ìš”:\n\n${query}\n\n${researchContext ? `ìˆ˜ì§‘ëœ ì •ë³´:\n${researchContext}\n\n` : ''}ìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let analysis = '';
  for await (const chunk of LLMService.streamChat(
    [systemMessage, ...state.messages, analysisPrompt],
    { tools: [], tool_choice: 'none' }
  )) {
    analysis += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Sequential] Analysis complete:', `${analysis.substring(0, 100)}...`);

  return {
    context: `# Analysis\n\n${analysis}`,
  };
}

/**
 * 2ë‹¨ê³„: ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½
 */
async function planNode(state: ChatState) {
  console.log('[Sequential] Step 2: Planning solution steps...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## ğŸ“‹ 2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½ ì¤‘ì…ë‹ˆë‹¤...\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì „ëµì  ê³„íš AIì…ë‹ˆë‹¤. ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

í¬ê´„ì ì¸ ë‹µë³€ìœ¼ë¡œ ì´ì–´ì§ˆ ë‹¨ê³„ ëª©ë¡(3-5ë‹¨ê³„)ì„ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì‘ì„±í•˜ì„¸ìš”.
ê° ë‹¨ê³„ëŠ” ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const planPrompt: Message = {
    id: 'plan-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:\n\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}`,
    created_at: Date.now(),
  };

  let plan = '';
  for await (const chunk of LLMService.streamChat([systemMessage, planPrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    plan += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Sequential] Plan complete:', `${plan.substring(0, 100)}...`);

  return {
    context: `${state.context}\n\n# Plan\n\n${plan}`,
  };
}

/**
 * 3ë‹¨ê³„: ë‹¨ê³„ë³„ ì‹¤í–‰
 */
async function executeNode(state: ChatState) {
  console.log('[Sequential] Step 3: Executing plan...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## âš™ï¸ 3ë‹¨ê³„: ê³„íš ì‹¤í–‰\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ìˆ˜ë¦½ëœ ê³„íšì„ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ì‹ ì¤‘í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ì„¸ë¶€ ì§€í–¥ì ì¸ AIì…ë‹ˆë‹¤.

ê° ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ì„œ ìƒì„¸í•œ ì¶”ë¡ ê³¼ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
ì² ì €í•˜ê²Œ ì—¬ëŸ¬ ê°ë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const executePrompt: Message = {
    id: 'execute-prompt',
    role: 'user',
    content: `ì´ ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ìƒì„¸íˆ ì‹¤í–‰í•˜ì„¸ìš”:\n\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}`,
    created_at: Date.now(),
  };

  let execution = '';
  for await (const chunk of LLMService.streamChat([systemMessage, executePrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    execution += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Sequential] Execution complete:', `${execution.substring(0, 100)}...`);

  return {
    context: `${state.context}\n\n# Execution\n\n${execution}`,
  };
}

/**
 * 4ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
 */
async function synthesizeNode(state: ChatState) {
  console.log('[Sequential] Step 4: Synthesizing final answer...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## âœ¨ 4ë‹¨ê³„: ìµœì¢… ë‹µë³€\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ìµœì¢… ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: createBaseSystemMessage(),
    created_at: Date.now(),
  };

  const synthesizePrompt: Message = {
    id: 'synthesize-prompt',
    role: 'user',
    content: `ìœ„ì˜ ëª¨ë“  ë¶„ì„, ê³„íš, ì‹¤í–‰ì„ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì ì¸ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

${state.context}

ì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}

ìœ„ ì‚¬ê³  ê³¼ì •ì˜ ëª¨ë“  í†µì°°ì„ í¬í•¨í•˜ëŠ” ëª…í™•í•˜ê³  ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let finalAnswer = '';
  const messageId = `msg-${Date.now()}`;

  for await (const chunk of LLMService.streamChat([systemMessage, synthesizePrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    finalAnswer += chunk;
    // Send each chunk to renderer via callback for real-time streaming (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  const assistantMessage: Message = {
    id: messageId,
    role: 'assistant',
    content: finalAnswer,
    created_at: Date.now(),
  };

  console.log('[Sequential] Final answer generated:', `${finalAnswer.substring(0, 100)}...`);

  return {
    messages: [assistantMessage],
  };
}

/**
 * Sequential Thinking Graph ìƒì„±
 */
export function createSequentialThinkingGraph() {
  const workflow = new StateGraph(ChatStateAnnotation)
    // ë…¸ë“œ ì¶”ê°€
    .addNode('research', researchNode)
    .addNode('analyze', analyzeNode)
    .addNode('plan', planNode)
    .addNode('execute', executeNode)
    .addNode('synthesize', synthesizeNode)
    // ìˆœì°¨ì  ì—£ì§€
    .addEdge('__start__', 'research')
    .addEdge('research', 'analyze')
    .addEdge('analyze', 'plan')
    .addEdge('plan', 'execute')
    .addEdge('execute', 'synthesize')
    .addEdge('synthesize', END);

  return workflow.compile();
}
