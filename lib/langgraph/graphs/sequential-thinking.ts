import { StateGraph, END } from '@langchain/langgraph';
import { ChatStateAnnotation, ChatState } from '../state';
import { LLMService } from '@/lib/llm/service';
import { Message } from '@/types';
import { createBaseSystemMessage } from '../utils/system-message';
import { emitStreamingChunk, getCurrentGraphConfig } from '@/lib/llm/streaming-callback';

/**
 * Sequential Thinking Graph
 *
 * ë‹¨ê³„ë³„ë¡œ ì‚¬ê³ í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê·¸ë˜í”„
 * 1. ë¬¸ì œ ë¶„ì„ (Analyze)
 * 2. ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½ (Plan)
 * 3. ê° ë‹¨ê³„ ì‹¤í–‰ (Execute)
 * 4. ìµœì¢… ë‹µë³€ ìƒì„± (Synthesize)
 */

/**
 * RAG ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜
 */
async function retrieveContextIfEnabled(query: string): Promise<string> {
  const config = getCurrentGraphConfig();
  if (!config?.enableRAG) {
    return '';
  }

  try {
    // Main Process ì „ìš© ë¡œì§
    if (typeof window !== 'undefined') {
      return '';
    }

    console.log('[Sequential] RAG enabled, retrieving documents...');
    const { vectorDBService } = await import('../../../electron/services/vectordb');
    const { databaseService } = await import('../../../electron/services/database');
    const { initializeEmbedding, getEmbeddingProvider } =
      await import('@/lib/vectordb/embeddings/client');

    const configStr = databaseService.getSetting('app_config');
    if (!configStr) {
      return '';
    }
    const appConfig = JSON.parse(configStr);
    if (!appConfig.embedding) {
      return '';
    }

    initializeEmbedding(appConfig.embedding);
    const embedder = getEmbeddingProvider();
    const queryEmbedding = await embedder.embed(query);
    const results = await vectorDBService.searchByVector(queryEmbedding, 5);

    if (results.length > 0) {
      console.log(`[Sequential] Found ${results.length} documents`);
      return results.map((doc, i) => `[ì°¸ê³  ë¬¸ì„œ ${i + 1}]\n${doc.content}`).join('\n\n');
    }
  } catch (error) {
    console.error('[Sequential] RAG retrieval failed:', error);
  }
  return '';
}

/**
 * 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„
 */
async function analyzeNode(state: ChatState) {
  console.log('[Sequential] Step 1: Analyzing problem...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n## ğŸ” 1ë‹¨ê³„: ë¬¸ì œ ë¶„ì„\n\n', state.conversationId);
  emitStreamingChunk('*ë¶„ì„ ì¤‘...*\n\n', state.conversationId);

  // RAG ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
  const query = state.messages[state.messages.length - 1].content;
  const ragContext = await retrieveContextIfEnabled(query);

  if (ragContext) {
    emitStreamingChunk(
      `\nğŸ“š **ê´€ë ¨ ë¬¸ì„œ ${ragContext.split('[ì°¸ê³  ë¬¸ì„œ').length - 1}ê°œë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.**\n\n`,
      state.conversationId
    );
  }

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ë¶„í•´í•˜ëŠ” ì‚¬ë ¤ ê¹Šì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œëŠ” ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì„ íŒŒì•…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤:
1. ì£¼ìš” ì§ˆë¬¸ ë˜ëŠ” ë¬¸ì œ
2. ê´€ë ¨ëœ í•µì‹¬ ê°œë…ë“¤
3. ë‹µë³€ì— í•„ìš”í•œ ì •ë³´

ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const analysisPrompt: Message = {
    id: 'analysis-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë¶„í•´í•˜ì„¸ìš”:\n\n${query}\n\n${ragContext ? `ì°¸ê³  ë¬¸ì„œ:\n${ragContext}\n\n` : ''}ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let analysis = '';
  for await (const chunk of LLMService.streamChat([
    systemMessage,
    ...state.messages,
    analysisPrompt,
  ])) {
    analysis += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Sequential] Analysis complete:', `${analysis.substring(0, 100)}...`);

  return {
    context: `# Analysis\n\n${analysis}`,
  };
}

/**
 * 2ë‹¨ê³„: ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½
 */
async function planNode(state: ChatState) {
  console.log('[Sequential] Step 2: Planning solution steps...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## ğŸ“‹ 2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½\n\n', state.conversationId);
  emitStreamingChunk('*ê³„íš ìˆ˜ë¦½ ì¤‘...*\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì „ëµì  ê³„íš AIì…ë‹ˆë‹¤. ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.

í¬ê´„ì ì¸ ë‹µë³€ìœ¼ë¡œ ì´ì–´ì§ˆ ë‹¨ê³„ ëª©ë¡(3-5ë‹¨ê³„)ì„ ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ì‘ì„±í•˜ì„¸ìš”.
ê° ë‹¨ê³„ëŠ” ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const planPrompt: Message = {
    id: 'plan-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¨ê³„ë³„ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”:\n\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}`,
    created_at: Date.now(),
  };

  let plan = '';
  for await (const chunk of LLMService.streamChat([systemMessage, planPrompt])) {
    plan += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Sequential] Plan complete:', `${plan.substring(0, 100)}...`);

  return {
    context: `${state.context}\n\n# Plan\n\n${plan}`,
  };
}

/**
 * 3ë‹¨ê³„: ë‹¨ê³„ë³„ ì‹¤í–‰
 */
async function executeNode(state: ChatState) {
  console.log('[Sequential] Step 3: Executing plan...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## âš™ï¸ 3ë‹¨ê³„: ê³„íš ì‹¤í–‰\n\n', state.conversationId);
  emitStreamingChunk('*ì‹¤í–‰ ì¤‘...*\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ì‹ ì¤‘í•˜ê²Œ ì‹¤í–‰í•˜ëŠ” ì„¸ë¶€ ì§€í–¥ì ì¸ AIì…ë‹ˆë‹¤.

ê° ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ì„œ ìƒì„¸í•œ ì¶”ë¡ ê³¼ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.
ì² ì €í•˜ê²Œ ì—¬ëŸ¬ ê°ë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const executePrompt: Message = {
    id: 'execute-prompt',
    role: 'user',
    content: `ì´ ê³„íšì˜ ê° ë‹¨ê³„ë¥¼ ìƒì„¸íˆ ì‹¤í–‰í•˜ì„¸ìš”:\n\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}`,
    created_at: Date.now(),
  };

  let execution = '';
  for await (const chunk of LLMService.streamChat([systemMessage, executePrompt])) {
    execution += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Sequential] Execution complete:', `${execution.substring(0, 100)}...`);

  return {
    context: `${state.context}\n\n# Execution\n\n${execution}`,
  };
}

/**
 * 4ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
 */
async function synthesizeNode(state: ChatState) {
  console.log('[Sequential] Step 4: Synthesizing final answer...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼ + ë¡œë”© í‘œì‹œ
  emitStreamingChunk('\n\n---\n\n## âœ¨ 4ë‹¨ê³„: ìµœì¢… ë‹µë³€\n\n', state.conversationId);
  emitStreamingChunk('*ë‹µë³€ ìƒì„± ì¤‘...*\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: createBaseSystemMessage(),
    created_at: Date.now(),
  };

  const synthesizePrompt: Message = {
    id: 'synthesize-prompt',
    role: 'user',
    content: `ìœ„ì˜ ëª¨ë“  ë¶„ì„, ê³„íš, ì‹¤í–‰ì„ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ì§ˆë¬¸ì— ëŒ€í•œ í¬ê´„ì ì¸ ìµœì¢… ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

${state.context}

ì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}

ìœ„ ì‚¬ê³  ê³¼ì •ì˜ ëª¨ë“  í†µì°°ì„ í¬í•¨í•˜ëŠ” ëª…í™•í•˜ê³  ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let finalAnswer = '';
  const messageId = `msg-${Date.now()}`;

  for await (const chunk of LLMService.streamChat([systemMessage, synthesizePrompt])) {
    finalAnswer += chunk;
    // Send each chunk to renderer via callback for real-time streaming (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  const assistantMessage: Message = {
    id: messageId,
    role: 'assistant',
    content: finalAnswer,
    created_at: Date.now(),
  };

  console.log('[Sequential] Final answer generated:', `${finalAnswer.substring(0, 100)}...`);

  return {
    messages: [assistantMessage],
  };
}

/**
 * Sequential Thinking Graph ìƒì„±
 */
export function createSequentialThinkingGraph() {
  const workflow = new StateGraph(ChatStateAnnotation)
    // ë…¸ë“œ ì¶”ê°€
    .addNode('analyze', analyzeNode)
    .addNode('plan', planNode)
    .addNode('execute', executeNode)
    .addNode('synthesize', synthesizeNode)
    // ìˆœì°¨ì  ì—£ì§€
    .addEdge('__start__', 'analyze')
    .addEdge('analyze', 'plan')
    .addEdge('plan', 'execute')
    .addEdge('execute', 'synthesize')
    .addEdge('synthesize', END);

  return workflow.compile();
}
