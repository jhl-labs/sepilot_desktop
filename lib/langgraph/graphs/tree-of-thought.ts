import { StateGraph, END } from '@langchain/langgraph';
import { Annotation } from '@langchain/langgraph';
import { Message } from '@/types';
import { LLMService } from '@/lib/llm/service';
import { createBaseSystemMessage } from '../utils/system-message';
import { emitStreamingChunk, getCurrentGraphConfig } from '@/lib/llm/streaming-callback';

/**
 * Tree of Thought Graph
 *
 * ì—¬ëŸ¬ ì‚¬ê³  ê²½ë¡œë¥¼ íƒìƒ‰í•˜ê³  í‰ê°€í•˜ì—¬ ìµœì„ ì˜ ë‹µë³€ì„ ìƒì„±
 * 1. ë¬¸ì œ ë¶„í•´ (Decompose)
 * 2. ë‹¤ì¤‘ ê²½ë¡œ ìƒì„± (Generate Branches)
 * 3. ê° ê²½ë¡œ í‰ê°€ (Evaluate)
 * 4. ìµœì„ ì˜ ê²½ë¡œ ì„ íƒ ë° ë‹µë³€ ìƒì„± (Select & Synthesize)
 */

/**
 * RAG ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜
 */
async function retrieveContextIfEnabled(query: string): Promise<string> {
  const config = getCurrentGraphConfig();
  if (!config?.enableRAG) {
    return '';
  }

  try {
    // Main Process ì „ìš© ë¡œì§
    if (typeof window !== 'undefined') {
      return '';
    }

    console.log('[ToT] RAG enabled, retrieving documents...');
    const { vectorDBService } = await import('../../../electron/services/vectordb');
    const { databaseService } = await import('../../../electron/services/database');
    const { initializeEmbedding, getEmbeddingProvider } =
      await import('@/lib/vectordb/embeddings/client');

    const configStr = databaseService.getSetting('app_config');
    if (!configStr) {
      return '';
    }
    const appConfig = JSON.parse(configStr);
    if (!appConfig.embedding) {
      return '';
    }

    initializeEmbedding(appConfig.embedding);
    const embedder = getEmbeddingProvider();
    const queryEmbedding = await embedder.embed(query);
    const results = await vectorDBService.searchByVector(queryEmbedding, 5);

    if (results.length > 0) {
      console.log(`[ToT] Found ${results.length} documents`);
      return results.map((doc, i) => `[ì°¸ê³  ë¬¸ì„œ ${i + 1}]\n${doc.content}`).join('\n\n');
    }
  } catch (error) {
    console.error('[ToT] RAG retrieval failed:', error);
  }
  return '';
}

/**
 * Tree of Thought State
 */
export const TreeOfThoughtStateAnnotation = Annotation.Root({
  messages: Annotation<Message[]>({
    reducer: (existing: Message[], updates: Message[]) => [...existing, ...updates],
    default: () => [],
  }),
  context: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  branches: Annotation<Array<{ id: string; content: string; score: number }>>({
    reducer: (_existing: any[], updates: any[]) => updates,
    default: () => [],
  }),
  selectedBranch: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  // conversationId: ë™ì‹œ ëŒ€í™” ì‹œ ìŠ¤íŠ¸ë¦¬ë° ê²©ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜
  conversationId: Annotation<string>({
    reducer: (_existing: string, update: string) => update || _existing,
    default: () => '',
  }),
});

export type TreeOfThoughtState = typeof TreeOfThoughtStateAnnotation.State;

/**
 * 1ë‹¨ê³„: ë¬¸ì œ ë¶„í•´
 */
async function decomposeNode(state: TreeOfThoughtState) {
  console.log('[ToT] Step 1: Decomposing problem...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n## ğŸŒ³ 1ë‹¨ê³„: ë¬¸ì œ ë¶„í•´\n\n', state.conversationId);

  // RAG ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
  const query = state.messages[state.messages.length - 1].content;
  const ragContext = await retrieveContextIfEnabled(query);

  if (ragContext) {
    emitStreamingChunk(
      `\nğŸ“š **ê´€ë ¨ ë¬¸ì„œ ${ragContext.split('[ì°¸ê³  ë¬¸ì„œ').length - 1}ê°œë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.**\n\n`,
      state.conversationId
    );
  }

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ë³µì¡í•œ ë¬¸ì œë¥¼ í•µì‹¬ ì¸¡ë©´ê³¼ ê³ ë ¤ì‚¬í•­ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ë¶„ì„ì  AIì…ë‹ˆë‹¤.

ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì„ íŒŒì•…í•˜ì„¸ìš”:
1. í•µì‹¬ ì§ˆë¬¸
2. ê³ ë ¤í•´ì•¼ í•  ì£¼ìš” ì¸¡ë©´ë“¤
3. ë‹µë³€ì— ëŒ€í•œ ê°€ëŠ¥í•œ ì ‘ê·¼ ë°©ì‹ë“¤

í¬ê´„ì ì´ë©´ì„œë„ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const decomposePrompt: Message = {
    id: 'decompose-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ì§ˆë¬¸ì„ í•µì‹¬ ì¸¡ë©´ë“¤ë¡œ ë¶„í•´í•˜ì„¸ìš”:\n\n${query}\n\n${ragContext ? `ì°¸ê³  ë¬¸ì„œ:\n${ragContext}\n\n` : ''}ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ ë¶„í•´í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let decomposition = '';
  for await (const chunk of LLMService.streamChat([
    systemMessage,
    ...state.messages,
    decomposePrompt,
  ])) {
    decomposition += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[ToT] Decomposition complete');

  return {
    context: decomposition,
  };
}

/**
 * 2ë‹¨ê³„: ë‹¤ì¤‘ ê²½ë¡œ ìƒì„± (3ê°œì˜ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹)
 */
async function generateBranchesNode(state: TreeOfThoughtState) {
  console.log('[ToT] Step 2: Generating multiple thought branches...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸŒ¿ 2ë‹¨ê³„: ë‹¤ì¤‘ ì‚¬ê³  ê²½ë¡œ ìƒì„±\n\n', state.conversationId);

  const branches: Array<{ id: string; content: string; score: number }> = [];

  // 3ê°€ì§€ ë‹¤ë¥¸ ì ‘ê·¼ ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±
  const approaches = [
    { name: 'ì‹¤ìš©ì  ì ‘ê·¼', desc: 'ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸ì— ì§‘ì¤‘' },
    { name: 'ì´ë¡ ì  ì ‘ê·¼', desc: 'ì´ë¡ ì  ì´í•´ì™€ ì›ì¹™ì— ì§‘ì¤‘' },
    { name: 'ê· í˜•ì  ì ‘ê·¼', desc: 'ì¥ë‹¨ì ì„ ê³ ë ¤í•œ ê· í˜• ì¡íŒ ê´€ì ì— ì§‘ì¤‘' },
  ];

  for (let i = 0; i < 3; i++) {
    // ê° ë¸Œëœì¹˜ ì‹œì‘ ì•Œë¦¼
    emitStreamingChunk(`\n### ğŸ”€ ê²½ë¡œ ${i + 1}: ${approaches[i].name}\n\n`, state.conversationId);

    const systemMessage: Message = {
      id: `system-branch-${i}`,
      role: 'system',
      content: `ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê´€ì ì„ ì œê³µí•˜ëŠ” ì‚¬ë ¤ ê¹Šì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

${approaches[i].desc}

ì•„ë˜ ë¶„í•´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ íŠ¹ì • ì ‘ê·¼ ë°©ì‹ì— ì§‘ì¤‘í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
      created_at: Date.now(),
    };

    const branchPrompt: Message = {
      id: `branch-prompt-${i}`,
      role: 'user',
      content: `ë¶„í•´:\n${state.context}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\n${approaches[i].desc}ì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”`,
      created_at: Date.now(),
    };

    let branchContent = '';
    for await (const chunk of LLMService.streamChat([systemMessage, branchPrompt])) {
      branchContent += chunk;
      // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
      emitStreamingChunk(chunk, state.conversationId);
    }

    branches.push({
      id: `branch-${i}`,
      content: branchContent,
      score: 0, // Will be evaluated in next step
    });

    console.log(`[ToT] Branch ${i + 1} generated`);
  }

  return {
    branches,
  };
}

/**
 * 3ë‹¨ê³„: ê° ê²½ë¡œ í‰ê°€
 */
async function evaluateBranchesNode(state: TreeOfThoughtState) {
  console.log('[ToT] Step 3: Evaluating branches...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## âš–ï¸ 3ë‹¨ê³„: ê²½ë¡œ í‰ê°€\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system-eval',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” í‰ê°€ AIì…ë‹ˆë‹¤.

ê° ë‹µë³€ì„ ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
1. ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„± (0-10)
2. ì™„ì „ì„± (0-10)
3. ëª…í™•ì„±ê³¼ ì¼ê´€ì„± (0-10)
4. í†µì°°ì˜ ê¹Šì´ (0-10)

ê° ë‹µë³€ì— ëŒ€í•´ ì´ì (0-40)ë§Œ ì œê³µí•˜ì„¸ìš”. í˜•ì‹: "ì ìˆ˜: X". ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const evaluatedBranches: Array<{ id: string; content: string; score: number }> = [];

  for (let idx = 0; idx < state.branches.length; idx++) {
    const branch = state.branches[idx];

    emitStreamingChunk(`\n### ğŸ“Š ê²½ë¡œ ${idx + 1} í‰ê°€ ì¤‘...\n\n`, state.conversationId);

    const evalPrompt: Message = {
      id: `eval-prompt-${idx}`,
      role: 'user',
      content: `ì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\ní‰ê°€í•  ë‹µë³€:\n${branch.content}\n\nì´ì (0-40)ì„ ì œê³µí•˜ì„¸ìš”:`,
      created_at: Date.now(),
    };

    let scoreText = '';
    for await (const chunk of LLMService.streamChat([systemMessage, evalPrompt])) {
      scoreText += chunk;
      // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
      emitStreamingChunk(chunk, state.conversationId);
    }

    // Extract score
    const match = scoreText.match(/(\d+)/);
    const score = match ? parseInt(match[1]) : 20; // Default to middle score if parsing fails

    console.log(`[ToT] Branch ${idx + 1} score: ${score}`);

    evaluatedBranches.push({
      ...branch,
      score,
    });
  }

  // Sort by score and select best
  const sortedBranches = evaluatedBranches.sort((a, b) => b.score - a.score);
  const selectedBranch = sortedBranches[0].content;

  emitStreamingChunk(
    `\n\n**ğŸ† ìµœê³  ì ìˆ˜ ê²½ë¡œ ì„ íƒë¨ (ì ìˆ˜: ${sortedBranches[0].score})**\n`,
    state.conversationId
  );

  console.log('[ToT] Best branch selected');

  return {
    branches: evaluatedBranches,
    selectedBranch,
  };
}

/**
 * 4ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
 */
async function synthesizeNode(state: TreeOfThoughtState) {
  console.log('[ToT] Step 4: Synthesizing final answer...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## âœ¨ 4ë‹¨ê³„: ìµœì¢… ë‹µë³€ í†µí•©\n\n', state.conversationId);

  const systemMessage: Message = {
    id: 'system-synth',
    role: 'system',
    content: createBaseSystemMessage(),
    created_at: Date.now(),
  };

  // ìƒìœ„ 2ê°œ ë¸Œëœì¹˜ë¥¼ í†µí•©
  const topBranches = state.branches
    .sort((a, b) => b.score - a.score)
    .slice(0, 2)
    .map((b, idx) => `### Approach ${idx + 1} (Score: ${b.score}):\n${b.content}`)
    .join('\n\n');

  const synthesizePrompt: Message = {
    id: 'synthesize-prompt',
    role: 'user',
    content: `ë‹¹ì‹ ì€ ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì ‘ê·¼ ë°©ì‹ì„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤.
ì´ì œ ìƒìœ„ ì ‘ê·¼ ë°©ì‹ë“¤ì˜ ìµœê³ ì˜ í†µì°°ì„ í¬ê´„ì ì´ê³  ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ìœ¼ë¡œ ì¢…í•©í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}

íƒìƒ‰ëœ ìƒìœ„ ì ‘ê·¼ ë°©ì‹ë“¤:
${topBranches}

ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ë“¤ì˜ ìµœê³ ì˜ ì¸¡ë©´ì„ í¬í•¨í•˜ëŠ” ìµœì¢…ì ì´ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let finalAnswer = '';
  const messageId = `msg-${Date.now()}`;

  for await (const chunk of LLMService.streamChat([systemMessage, synthesizePrompt])) {
    finalAnswer += chunk;
    // Send each chunk to renderer via callback for real-time streaming (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  const assistantMessage: Message = {
    id: messageId,
    role: 'assistant',
    content: finalAnswer,
    created_at: Date.now(),
  };

  console.log('[ToT] Final answer synthesized');

  return {
    messages: [assistantMessage],
  };
}

/**
 * Tree of Thought Graph ìƒì„±
 */
export function createTreeOfThoughtGraph() {
  const workflow = new StateGraph(TreeOfThoughtStateAnnotation)
    // ë…¸ë“œ ì¶”ê°€
    .addNode('decompose', decomposeNode)
    .addNode('generate_branches', generateBranchesNode)
    .addNode('evaluate', evaluateBranchesNode)
    .addNode('synthesize', synthesizeNode)
    // ìˆœì°¨ì  ì—£ì§€
    .addEdge('__start__', 'decompose')
    .addEdge('decompose', 'generate_branches')
    .addEdge('generate_branches', 'evaluate')
    .addEdge('evaluate', 'synthesize')
    .addEdge('synthesize', END);

  return workflow.compile();
}
