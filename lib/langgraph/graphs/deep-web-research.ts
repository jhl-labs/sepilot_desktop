import { StateGraph, END } from '@langchain/langgraph';
import { AgentStateAnnotation, AgentState } from '../state';
import { generateWithToolsNode } from '../nodes/generate';
import { toolsNode } from '../nodes/tools';
import type { Message } from '@/types';
import { emitStreamingChunk, getCurrentGraphConfig } from '@/lib/llm/streaming-callback';
import { LLMService } from '@/lib/llm/service';

const MAX_ITERATIONS = 3;

// RAG í—¬í¼ í•¨ìˆ˜
async function retrieveContextIfEnabled(query: string): Promise<string> {
  const config = getCurrentGraphConfig();
  if (!config?.enableRAG) {
    return '';
  }

  try {
    if (typeof window !== 'undefined') {
      return '';
    }

    console.log('[DeepWebResearch] RAG enabled, retrieving documents...');
    const { vectorDBService } = await import('../../../electron/services/vectordb');
    const { databaseService } = await import('../../../electron/services/database');
    const { initializeEmbedding, getEmbeddingProvider } = 
      await import('@/lib/vectordb/embeddings/client');

    const configStr = databaseService.getSetting('app_config');
    if (!configStr) return '';
    const appConfig = JSON.parse(configStr);
    if (!appConfig.embedding) return '';

    initializeEmbedding(appConfig.embedding);
    const embedder = getEmbeddingProvider();
    const queryEmbedding = await embedder.embed(query);
    const results = await vectorDBService.searchByVector(queryEmbedding, 5);

    if (results.length > 0) {
      console.log(`[DeepWebResearch] Found ${results.length} documents`);
      return results.map((doc, i) => `[ì°¸ê³  ë¬¸ì„œ ${i + 1}]
${doc.content}`).join('\n\n');
    }
  } catch (error) {
    console.error('[DeepWebResearch] RAG retrieval failed:', error);
  }
  return '';
}

/**
 * 1ë‹¨ê³„: ê²€ìƒ‰ ê³„íš ìˆ˜ë¦½ (Plan Node)
 */
async function planNode(state: AgentState): Promise<Partial<AgentState>> {
  const iteration = (state.planningNotes as any)?.iteration || 0;
  const isFirstStep = iteration === 0;
  const query = state.messages[state.messages.length - 1].content;

  // RAG Context (ì²« í„´ì—ë§Œ ìˆ˜í–‰)
  let ragContext = '';
  if (isFirstStep) {
    ragContext = await retrieveContextIfEnabled(query);
  }

  console.log(`[DeepWebResearch] Planning Step (Iter ${iteration + 1})`);

  if (isFirstStep) {
    emitStreamingChunk('\n\n## ğŸ§  ì‹¬ì¸µ ì›¹ ì—°êµ¬ ì‹œì‘\n\n', state.conversationId);
  } else {
    emitStreamingChunk(`\n\n### ğŸ”„ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (ë‹¨ê³„ ${iteration + 1}/${MAX_ITERATIONS})\n\n`, state.conversationId);
  }

  // ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ë¬¸ìì—´ ìƒì„±
  const toolResults = state.toolResults || [];
  // ë„ˆë¬´ ê¸¸ë©´ ìµœê·¼ ê²ƒë§Œ? ì¼ë‹¨ ì „ì²´ í¬í•¨ (LLM Context Windowê°€ í¬ë‹¤ê³  ê°€ì •)
  const previousResults = toolResults
    .map((r, i) => `[ê²€ìƒ‰ ê²°ê³¼ ${i + 1}] (${r.toolName}):\n${typeof r.result === 'string' ? r.result.substring(0, 1000) : JSON.stringify(r.result).substring(0, 1000)}...`)
    .join('\n\n');

  const systemMessage: Message = {
    id: 'system-plan',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ 'Deep Web Researcher'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ê¹Šì´ ìˆê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ê¸° ìœ„í•´ ë‹¨ê³„ë³„ë¡œ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

í˜„ì¬ ë‹¨ê³„: ${iteration + 1} / ${MAX_ITERATIONS}

[ì§€ì‹œì‚¬í•­]
1. í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬, ë” í•„ìš”í•œ ì •ë³´ê°€ ë¬´ì—‡ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.
2. ì •ë³´ê°€ ì¶©ë¶„í•˜ë‹¤ë©´ 'queries'ë¥¼ ë¹ˆ ë°°ì—´ []ë¡œ ë°˜í™˜í•˜ì—¬ ê²€ìƒ‰ì„ ì¢…ë£Œí•˜ì„¸ìš”.
3. ë” ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´, Tavily ê²€ìƒ‰ ë„êµ¬ë¥¼ ìœ„í•œ ìµœì ì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.
4. **ì¤‘ìš”**: 'tavily_search' ë„êµ¬ì˜ íŒŒë¼ë¯¸í„°ëŠ” ì˜¤ì§ {"query": "...", "max_results": 5} í˜•íƒœë§Œ í—ˆìš©ë©ë‹ˆë‹¤. 'topn' ë“± ë‹¤ë¥¸ í‚¤ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
5. í•œ ë²ˆì— ìµœëŒ€ 3ê°œì˜ ë³‘ë ¬ ì¿¼ë¦¬ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
6. ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

[JSON í˜•ì‹ ì˜ˆì‹œ]
{
  "thought": "ì‚¬ìš©ìê°€ ...ì— ëŒ€í•´ ë¬¼ì—ˆìœ¼ë¯€ë¡œ, ...ì— ëŒ€í•œ ìµœì‹  í†µê³„ë¥¼ ê²€ìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.",
  "queries": [
    {"tool_name": "tavily_search", "parameters": {"query": "Gemini 3.0 benchmark results", "max_results": 5}},
    {"tool_name": "tavily_search", "parameters": {"query": "Gemini 3.0 technical report", "max_results": 5}}
  ]
}`,
    created_at: Date.now(),
  };

  const planPrompt: Message = {
    id: 'plan-prompt',
    role: 'user',
    content: `ì›ë³¸ ì§ˆë¬¸: ${query}

${ragContext ? `[ì‚¬ì „ RAG ì •ë³´]\n${ragContext}\n\n` : ''}
[í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ ì •ë³´]
${previousResults || '(ì—†ìŒ)'}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ê²€ìƒ‰ ê³„íšì„ JSONìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.`, 
    created_at: Date.now(),
  };

  let planOutput = '';
  // ë„êµ¬ í˜¸ì¶œ ì—†ì´ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•´ tools: []
  for await (const chunk of LLMService.streamChat([systemMessage, planPrompt], { tools: [], tool_choice: 'none' })) {
    // ê³„íš ìƒì„± ê³¼ì •ì€ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ì§€ ì•Šê±°ë‚˜ ê°„ëµíˆë§Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ
    // ì—¬ê¸°ì„œëŠ” ë””ë²„ê¹…ì„ ìœ„í•´ ë¡œê·¸ë¡œë§Œ ë‚¨ê¸°ê³ , ì‹¤ì œ ê³„íšëœ ì¿¼ë¦¬ëŠ” ì•„ë˜ì—ì„œ ì¶œë ¥
    planOutput += chunk;
  }

  console.log('[DeepWebResearch] Plan Output:', planOutput);

  // íŒŒì‹±
  let plannedQueries: any[] = [];
  let thought = '';

  try {
    // JSON ì¶”ì¶œ
    const jsonMatch = planOutput.match(/\{[\s\S]*\}/);
    if (jsonMatch) {
      const parsed = JSON.parse(jsonMatch[0]);
      plannedQueries = parsed.queries || [];
      thought = parsed.thought || '';
    } else {
      console.warn('[DeepWebResearch] Failed to find JSON in plan output');
    }
  } catch (e) {
    console.error('[DeepWebResearch] JSON parse error:', e);
  }

  // í™”ë©´ì— ê³„íš ì•Œë¦¼
  if (thought) {
    emitStreamingChunk(`ğŸ¤” **ìƒê°:** ${thought}\n\n`, state.conversationId);
  }
  if (plannedQueries.length > 0) {
    const queryList = plannedQueries.map(q => `- "${q.parameters.query}"`).join('\n');
    emitStreamingChunk(`ğŸ“ **ê²€ìƒ‰ ê³„íš:**\n${queryList}\n\n`, state.conversationId);
  } else {
    emitStreamingChunk(`âœ… **ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ. ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.**\n\n`, state.conversationId);
  }

  return {
    messages: state.messages, // ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ëŠ” ìœ ì§€
    planningNotes: {
      queries: plannedQueries,
      iteration: iteration + 1,
      thought
    }
  };
}

/**
 * 2ë‹¨ê³„: ì›¹ ê²€ìƒ‰ ì‹¤í–‰ (Search Node)
 */
async function searchNode(state: AgentState): Promise<Partial<AgentState>> {
  const notes = state.planningNotes as any;
  const queries = notes?.queries || [];

  if (queries.length === 0) {
    return {};
  }

  console.log(`[DeepWebResearch] Executing ${queries.length} searches...`);
  emitStreamingChunk(`ğŸš€ **ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...**\n`, state.conversationId);

  let newToolResults: any[] = [];

  // ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•´ Promise.all ì‚¬ìš©
  // toolsNodeëŠ” tool_calls ë©”ì‹œì§€ë¥¼ ë°›ì•„ ì²˜ë¦¬í•˜ë¯€ë¡œ, ì§ì ‘ MCPServerManagerë‚˜ builtin toolsë¥¼ í˜¸ì¶œí•˜ëŠ” ê²Œ ë‚˜ì„ ìˆ˜ ìˆìŒ.
  // í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì¼ê´€ì„±ì„ ìœ„í•´ toolsNode ë¡œì§ì„ í‰ë‚´ë‚´ê±°ë‚˜ ì¬ì‚¬ìš©.
  // ì—¬ê¸°ì„œëŠ” ê° ì¿¼ë¦¬ë¥¼ ê°œë³„ tool callë¡œ ë§Œë“¤ì–´ ì²˜ë¦¬.

  // ì£¼ì˜: ì‹¤ì œ ë„êµ¬ ì‹¤í–‰ì€ toolsNodeë¥¼ í†µí•´ì•¼ í™œë™ ë¡œê·¸ ë“±ì´ ë‚¨ìŒ.
  // í•˜ì§€ë§Œ toolsNodeëŠ” state.messagesì˜ ë§ˆì§€ë§‰ ë©”ì‹œì§€ì˜ tool_callsë¥¼ ë´…ë‹ˆë‹¤.
  // ì—¬ê¸°ì„œëŠ” ì„ì‹œ ë©”ì‹œì§€ë¥¼ ë§Œë“¤ì–´ toolsNodeì— ë„˜ê¹ë‹ˆë‹¤.

  const tempToolCalls = queries.map((q: any, idx: number) => ({
    id: `call-${Date.now()}-${idx}`,
    name: q.tool_name,
    arguments: q.parameters
  }));

  const tempMessage: Message = {
    id: `temp-tool-msg-${Date.now()}`,
    role: 'assistant',
    content: '',
    tool_calls: tempToolCalls,
    created_at: Date.now()
  };

  // toolsNode í˜¸ì¶œ
  try {
    const resultState = await toolsNode({
      ...state,
      messages: [...state.messages, tempMessage]
    } as any); // AgentState í˜¸í™˜ì„±

    newToolResults = resultState.toolResults || [];
    
    // ê²°ê³¼ ë¡œê¹…
    for (const res of newToolResults) {
        if (res.error) {
            emitStreamingChunk(`âŒ **ì‹¤íŒ¨:** ${res.toolName} - ${res.error}\n`, state.conversationId);
        } else {
            // ê²°ê³¼ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤„ ìˆ˜ë„ ìˆìŒ
            emitStreamingChunk(`âœ… **ì™„ë£Œ:** ${res.toolName}\n`, state.conversationId);
        }
    }

      } catch (e: any) {
        emitStreamingChunk(`âŒ **ê²€ìƒ‰ ì‹¤íŒ¨:** ${e.message}\n`, state.conversationId);
        toolResults.push({ error: e.message, toolName: 'tavily_search' });

        // Rate Limitì´ë‚˜ ì¹˜ëª…ì  ì—ëŸ¬ ê°ì§€
        if (
          e.message.includes('429') ||
          e.message.toLowerCase().includes('limit') ||
          e.message.toLowerCase().includes('quota') ||
          e.message.toLowerCase().includes('unauthorized')
        ) {
          emitStreamingChunk(
            `âš ï¸ **ê²€ìƒ‰ ì œí•œ ê°ì§€:** ì¶”ê°€ ê²€ìƒ‰ì„ ì¤‘ë‹¨í•˜ê³  í˜„ì¬ ì •ë³´ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n`,
            state.conversationId
          );
          return {
            toolResults: [...(state.toolResults || []), ...toolResults],
            planningNotes: { ...(state.planningNotes as object), forceSynthesize: true },
          };
        }
      }
    }
  }

  return {
    // ê¸°ì¡´ toolResultsì— ëˆ„ì 
    toolResults: [...(state.toolResults || []), ...toolResults],
  };
}

/**
 * ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
 */
function checkPlan(state: AgentState) {
  const notes = state.planningNotes as any;

  // ê°•ì œ ì¢…ë£Œ í”Œë˜ê·¸ í™•ì¸
  if (notes?.forceSynthesize) {
    return 'synthesize';
  }

  // ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ (Synthesize)
  if (!notes || !notes.queries || notes.queries.length === 0) {
    return 'synthesize';
  }
  
  // ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ì¢…ë£Œ
  if (notes.iteration > MAX_ITERATIONS) {
    return 'synthesize';
  }
  
  return 'search';
}

/**
 * 3ë‹¨ê³„: ê²°ê³¼ ì¢…í•© ë° ë‹µë³€ ìƒì„± (Synthesize Node)
 */
async function synthesizeNode(state: AgentState): Promise<Partial<AgentState>> {
  console.log('[DeepWebResearch] Step 3: Synthesizing answer...');
  emitStreamingChunk('\n\n## âœ¨ ìµœì¢… ë‹µë³€ ìƒì„±\n\n', state.conversationId);
  emitStreamingChunk('**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ìˆ˜ì§‘ëœ ë°©ëŒ€í•œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•©ë‹ˆë‹¤...\n\n', state.conversationId);

  const allSearchOutputs = state.toolResults?.map(r => {
      const content = typeof r.result === 'string' ? r.result : JSON.stringify(r.result);
      return `[ì¶œì²˜: ${r.toolName}]\n${content.substring(0, 2000)}... (ìƒëµë¨)`; // ì»¨í…ìŠ¤íŠ¸ ì œí•œ ê³ ë ¤
  }).join('\n\n---\n\n') || 'ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ';
  
  const systemMessage: Message = {
    id: 'system-synth',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í¬ê´„ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.
ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì •í™•í•˜ê²Œ ìš”ì•½í•˜ê³ , ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ë¶ˆì¶©ë¶„í•˜ë‹¤ë©´(ê²€ìƒ‰ ì‹¤íŒ¨, ì œí•œ ë“±), ë‹¹ì‹ ì˜ ë‚´ë¶€ ì§€ì‹ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ê³  ê²€ìƒ‰ì— ì–´ë ¤ì›€ì´ ìˆì—ˆìŒì„ ì‚¬ìš©ìì—ê²Œ ì•Œë¦¬ì„¸ìš”.`,
    created_at: Date.now(),
  };
  
  const synthesizePrompt: Message = {
    id: 'synth-prompt',
    role: 'user',
    content: `ì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}

[ìˆ˜ì§‘ëœ ì—°êµ¬ ìë£Œ]
${allSearchOutputs}

ìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.`, 
    created_at: Date.now(),
  };

  let finalAnswer = '';
  // ì‚¬ê³  ëª¨ë¸ê³¼ ë‹¬ë¦¬ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ { tools: [], tool_choice: 'none' }
  for await (const chunk of LLMService.streamChat(
    [systemMessage, synthesizePrompt],
    { tools: [], tool_choice: 'none' }
  )) {
    finalAnswer += chunk;
    emitStreamingChunk(chunk, state.conversationId);
  }

  // í›„ì† ì§ˆë¬¸ ìƒì„± (Perplexity ìŠ¤íƒ€ì¼)
  let followUpQuestions = '';
  if (finalAnswer) {
    emitStreamingChunk('\n\n---\n### ğŸ’¡ ì¶”ì²œ í›„ì† ì§ˆë¬¸\n', state.conversationId);

    const tempAssistantMessage: Message = {
      id: 'temp-assistant',
      role: 'assistant',
      content: finalAnswer,
      created_at: Date.now(),
    };

    const followUpPrompt: Message = {
      id: 'follow-up-prompt',
      role: 'user',
      content: `ìœ„ ë‹µë³€ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì´ì–´ì„œ ê¶ê¸ˆí•´í•  ë§Œí•œ "ì¶”ì²œ í›„ì† ì§ˆë¬¸" 3ê°€ì§€ë¥¼ í•œêµ­ì–´ë¡œ ì œì•ˆí•´ì£¼ì„¸ìš”.\nì§ˆë¬¸ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì‘ì„±í•˜ì„¸ìš”. (ì„¤ëª… ë¶ˆí•„ìš”)`,
      created_at: Date.now(),
    };

    try {
      for await (const chunk of LLMService.streamChat(
        [systemMessage, synthesizePrompt, tempAssistantMessage, followUpPrompt],
        { tools: [], tool_choice: 'none' }
      )) {
        followUpQuestions += chunk;
        emitStreamingChunk(chunk, state.conversationId);
      }
    } catch (e) {
      console.error('[DeepWebResearch] Failed to generate follow-up questions', e);
    }
  }

  const finalContent = followUpQuestions
    ? `${finalAnswer}\n\n---\n### ğŸ’¡ ì¶”ì²œ í›„ì† ì§ˆë¬¸\n${followUpQuestions}`
    : finalAnswer;

  const assistantMessage: Message = {
    id: `msg-${Date.now()}`,
    role: 'assistant',
    content: finalContent,
    created_at: Date.now(),
  };

  console.log('[DeepWebResearch] Final answer synthesized');

  return {
    messages: [assistantMessage],
  };
}

/**
 * Deep Web Research Graph ìƒì„±
 */
export function createDeepWebResearchGraph() {
  const workflow = new StateGraph(AgentStateAnnotation)
    .addNode('plan', planNode)
    .addNode('search', searchNode)
    .addNode('synthesize', synthesizeNode)
    
    .addEdge('__start__', 'plan')
    
    // plan -> checkPlan -> (search | synthesize)
    .addConditionalEdges('plan', checkPlan, {
        search: 'search',
        synthesize: 'synthesize'
    })
    
    // search -> plan (Loop)
    .addEdge('search', 'plan')
    
    .addEdge('synthesize', END);

  return workflow.compile();
}