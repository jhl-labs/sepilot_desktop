import { StateGraph, END } from '@langchain/langgraph';
import { Annotation } from '@langchain/langgraph';
import { Message } from '@/types';
import { LLMService } from '@/lib/llm/service';
import { createBaseSystemMessage } from '../utils/system-message';
import { emitStreamingChunk, getCurrentGraphConfig } from '@/lib/llm/streaming-callback';

/**
 * Deep Thinking Graph
 *
 * Sequential Thinkingê³¼ Tree of Thoughtë¥¼ ê²°í•©í•œ ê°€ì¥ ê¹Šì€ ì‚¬ê³  ë°©ì‹
 *
 * í”„ë¡œì„¸ìŠ¤:
 * 1. ì´ˆê¸° ë¶„ì„ (Initial Analysis)
 * 2. ë‹¤ì¤‘ ê´€ì  íƒìƒ‰ (Multi-Perspective Exploration)
 * 3. ê° ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ (Deep Analysis per Perspective)
 * 4. í†µí•© ë° ê²€ì¦ (Integration & Verification)
 * 5. ìµœì¢… ë‹µë³€ ìƒì„± (Final Synthesis)
 */

/**
 * RAG ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜
 */
async function retrieveContextIfEnabled(query: string): Promise<string> {
  const config = getCurrentGraphConfig();
  if (!config?.enableRAG) {
    return '';
  }

  try {
    // Main Process ì „ìš© ë¡œì§
    if (typeof window !== 'undefined') {
      return '';
    }

    console.log('[Deep] RAG enabled, retrieving documents...');
    const { vectorDBService } = await import('../../../electron/services/vectordb');
    const { databaseService } = await import('../../../electron/services/database');
    const { initializeEmbedding, getEmbeddingProvider } =
      await import('@/lib/vectordb/embeddings/client');

    const configStr = databaseService.getSetting('app_config');
    if (!configStr) {
      return '';
    }
    const appConfig = JSON.parse(configStr);
    if (!appConfig.embedding) {
      return '';
    }

    initializeEmbedding(appConfig.embedding);
    const embedder = getEmbeddingProvider();
    const queryEmbedding = await embedder.embed(query);
    const results = await vectorDBService.searchByVector(queryEmbedding, 5);

    if (results.length > 0) {
      console.log(`[Deep] Found ${results.length} documents`);
      return results.map((doc, i) => `[ì°¸ê³  ë¬¸ì„œ ${i + 1}]\n${doc.content}`).join('\n\n');
    }
  } catch (error) {
    console.error('[Deep] RAG retrieval failed:', error);
  }
  return '';
}

export const DeepThinkingStateAnnotation = Annotation.Root({
  messages: Annotation<Message[]>({
    reducer: (existing: Message[], updates: Message[]) => [...existing, ...updates],
    default: () => [],
  }),
  initialAnalysis: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  perspectives: Annotation<
    Array<{ id: string; name: string; content: string; deepAnalysis: string }>
  >({
    reducer: (_existing: any[], updates: any[]) => updates,
    default: () => [],
  }),
  integration: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  verification: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  // conversationId: ë™ì‹œ ëŒ€í™” ì‹œ ìŠ¤íŠ¸ë¦¬ë° ê²©ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜
  conversationId: Annotation<string>({
    reducer: (_existing: string, update: string) => update || _existing,
    default: () => '',
  }),
});

export type DeepThinkingState = typeof DeepThinkingStateAnnotation.State;

/**
 * 1ë‹¨ê³„: ì´ˆê¸° ë¶„ì„
 */
async function initialAnalysisNode(state: DeepThinkingState) {
  console.log('[Deep] Step 1/5: Initial comprehensive analysis...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n## ğŸ§  1ë‹¨ê³„: ì´ˆê¸° ì‹¬ì¸µ ë¶„ì„ (1/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ë¬¸ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  // RAG ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
  const query = state.messages[state.messages.length - 1].content;
  const ragContext = await retrieveContextIfEnabled(query);

  if (ragContext) {
    emitStreamingChunk(
      `\nğŸ“š **ê´€ë ¨ ë¬¸ì„œ ${ragContext.split('[ì°¸ê³  ë¬¸ì„œ').length - 1}ê°œë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.**\n\n`,
      state.conversationId
    );
  }

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ í¬ê´„ì ì¸ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê³ ë„ë¡œ ë¶„ì„ì ì¸ AIì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. í•µì‹¬ ì§ˆë¬¸ì„ ê¹Šì´ ì´í•´í•˜ê¸°
2. ëª¨ë“  ê´€ë ¨ ì¸¡ë©´ê³¼ ì°¨ì› íŒŒì•…í•˜ê¸°
3. ì´ ì§ˆë¬¸ì´ ë³µì¡í•˜ê±°ë‚˜ ë¯¸ë¬˜í•œ ì´ìœ  ê³ ë ¤í•˜ê¸°
4. íƒìƒ‰í•  ê°€ì¹˜ê°€ ìˆëŠ” ê´€ì  ê²°ì •í•˜ê¸°

ì² ì €í•˜ê³  ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const analysisPrompt: Message = {
    id: 'analysis-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í¬ê´„ì ì¸ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:\n\n${query}\n\n${ragContext ? `ì°¸ê³  ë¬¸ì„œ:\n${ragContext}\n\n` : ''}ìœ„ ì°¸ê³  ë¬¸ì„œë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let analysis = '';
  for await (const chunk of LLMService.streamChat([
    systemMessage,
    ...state.messages,
    analysisPrompt,
  ])) {
    analysis += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Deep] Initial analysis complete');

  return {
    initialAnalysis: analysis,
  };
}

/**
 * 2ë‹¨ê³„: ë‹¤ì¤‘ ê´€ì  íƒìƒ‰
 */
async function explorePerspectivesNode(state: DeepThinkingState) {
  console.log('[Deep] Step 2/5: Exploring multiple perspectives...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸ”­ 2ë‹¨ê³„: ë‹¤ì¤‘ ê´€ì  íƒìƒ‰ (2/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ íƒìƒ‰ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  const perspectiveTypes = [
    { name: 'ë¶„ì„ì  ê´€ì ', focus: 'ë…¼ë¦¬ì  ì¶”ë¡ , ì‚¬ì‹¤, ë°ì´í„°, ì²´ê³„ì  ë¶„ì„' },
    { name: 'ì‹¤ìš©ì  ê´€ì ', focus: 'ì‹¤ì œ ì ìš©, ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸, ì‹¤ìš©ì  í•´ê²°ì±…' },
    { name: 'ë¹„íŒì  ê´€ì ', focus: 'ì ì¬ì  ë¬¸ì œ, í•œê³„, ë°˜ë¡ , ì—£ì§€ ì¼€ì´ìŠ¤' },
    { name: 'ì°½ì˜ì  ê´€ì ', focus: 'í˜ì‹ ì  ì•„ì´ë””ì–´, ëŒ€ì•ˆì  ì ‘ê·¼, ë¹„ì „í†µì  ì‚¬ê³ ' },
  ];

  const perspectives: Array<{ id: string; name: string; content: string; deepAnalysis: string }> =
    [];

  for (const type of perspectiveTypes) {
    // ê° ê´€ì  ì‹œì‘ ì•Œë¦¼
    emitStreamingChunk(`\n### ğŸ‘ï¸ ${type.name}\n\n`, state.conversationId);

    const systemMessage: Message = {
      id: `system-${type.name}`,
      role: 'system',
      content: `ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ëŒ€í•´ ${type.name} ê´€ì ì„ íƒìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì§‘ì¤‘ ì˜ì—­: ${type.focus}

ì´ˆê¸° ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì´ íŠ¹ì • ê´€ì ì—ì„œ í†µì°°ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
      created_at: Date.now(),
    };

    const perspectivePrompt: Message = {
      id: `perspective-${type.name}`,
      role: 'user',
      content: `ì´ˆê¸° ë¶„ì„:\n${state.initialAnalysis}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\n${type.name}ì—ì„œ ì´ ì§ˆë¬¸ì„ íƒìƒ‰í•˜ì„¸ìš”:`,
      created_at: Date.now(),
    };

    let content = '';
    for await (const chunk of LLMService.streamChat([systemMessage, perspectivePrompt])) {
      content += chunk;
      // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
      emitStreamingChunk(chunk, state.conversationId);
    }

    console.log(`[Deep] ${type.name} perspective explored`);

    perspectives.push({
      id: type.name.toLowerCase(),
      name: type.name,
      content,
      deepAnalysis: '', // Will be filled in next step
    });
  }

  return {
    perspectives,
  };
}

/**
 * 3ë‹¨ê³„: ê° ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„
 */
async function deepAnalysisNode(state: DeepThinkingState) {
  console.log('[Deep] Step 3/5: Performing deep analysis on each perspective...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸ”¬ 3ë‹¨ê³„: ê´€ì ë³„ ì‹¬í™” ë¶„ì„ (3/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ê° ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  const deepAnalyzedPerspectives: Array<{
    id: string;
    name: string;
    content: string;
    deepAnalysis: string;
  }> = [];

  for (const perspective of state.perspectives) {
    // ê° ì‹¬í™” ë¶„ì„ ì‹œì‘ ì•Œë¦¼
    emitStreamingChunk(`\n### ğŸ” ${perspective.name} ì‹¬í™” ë¶„ì„\n\n`, state.conversationId);

    const systemMessage: Message = {
      id: `system-deep-${perspective.id}`,
      role: 'system',
      content: `ë‹¹ì‹ ì€ ${perspective.name}ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë” ê¹Šì´ ê²€í† í•˜ê¸°
2. í•¨ì˜ì™€ ê²°ê³¼ ê³ ë ¤í•˜ê¸°
3. ì•„ì´ë””ì–´ ì—°ê²° ë° íŒ¨í„´ íŒŒì•…í•˜ê¸°
4. ë…¼ì¦ì„ ì¶”ë¡ ìœ¼ë¡œ ê°•í™”í•˜ê¸°

ìƒì„¸í•˜ê³  ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
      created_at: Date.now(),
    };

    const deepAnalysisPrompt: Message = {
      id: `deep-analysis-${perspective.id}`,
      role: 'user',
      content: `${perspective.name}:\n${perspective.content}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nì´ ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:`,
      created_at: Date.now(),
    };

    let deepAnalysis = '';
    for await (const chunk of LLMService.streamChat([systemMessage, deepAnalysisPrompt])) {
      deepAnalysis += chunk;
      // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
      emitStreamingChunk(chunk, state.conversationId);
    }

    console.log(`[Deep] ${perspective.name} perspective deeply analyzed`);

    deepAnalyzedPerspectives.push({
      ...perspective,
      deepAnalysis,
    });
  }

  return {
    perspectives: deepAnalyzedPerspectives,
  };
}

/**
 * 4ë‹¨ê³„: í†µí•© ë° ê²€ì¦
 */
async function integrateAndVerifyNode(state: DeepThinkingState) {
  console.log('[Deep] Step 4/5: Integrating perspectives and verifying...');

  // í†µí•© ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸ”— 4ë‹¨ê³„: í†µí•© ë° ê²€ì¦ (4/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ê´€ì ë“¤ì„ í†µí•©í•˜ê³  ê²°ê³¼ì˜ ìœ íš¨ì„±ì„ ê²€ì¦ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );
  emitStreamingChunk('### ğŸ“¦ ê´€ì  í†µí•©\n\n', state.conversationId);

  const systemMessage1: Message = {
    id: 'system-integrate',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì—¬ëŸ¬ ê´€ì ì˜ í†µì°°ì„ ì¼ê´€ëœ ì´í•´ë¡œ í†µí•©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. ê´€ì ë“¤ ê°„ì˜ ê³µí†µ ì£¼ì œ íŒŒì•…í•˜ê¸°
2. ëª¨ìˆœ í•´ê²°í•˜ê¸°
3. ìƒí˜¸ë³´ì™„ì  í†µì°° ì¢…í•©í•˜ê¸°
4. í¬ê´„ì  ì´í•´ êµ¬ì¶•í•˜ê¸°

ì² ì €í•˜ê³  ì„¬ì„¸í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const allPerspectives = state.perspectives
    .map((p) => `### ${p.name}:\n${p.content}\n\n#### Deep Analysis:\n${p.deepAnalysis}`)
    .join('\n\n---\n\n');

  const integratePrompt: Message = {
    id: 'integrate-prompt',
    role: 'user',
    content: `ì´ˆê¸° ë¶„ì„:\n${state.initialAnalysis}\n\nëª¨ë“  ê´€ì :\n${allPerspectives}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nëª¨ë“  í†µì°°ì„ ì¼ê´€ëœ ì´í•´ë¡œ í†µí•©í•˜ì„¸ìš”:`,
    created_at: Date.now(),
  };

  let integration = '';
  for await (const chunk of LLMService.streamChat([systemMessage1, integratePrompt])) {
    integration += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Deep] Integration complete, now verifying...');

  // ê²€ì¦ ë‹¨ê³„ ì•Œë¦¼
  emitStreamingChunk('\n\n### âœ… ê²€ì¦ ë‹¨ê³„\n\n', state.conversationId);

  const systemMessage2: Message = {
    id: 'system-verify',
    role: 'system',
    content: `ë‹¹ì‹ ì€ í†µí•©ëœ ì´í•´ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. ì§ˆë¬¸ì˜ ëª¨ë“  ì¸¡ë©´ì´ ë‹¤ë¤„ì¡ŒëŠ”ì§€ í™•ì¸í•˜ê¸°
2. ë¹ˆí‹ˆì´ë‚˜ ì•½ì  íŒŒì•…í•˜ê¸°
3. ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ë³´í•˜ê¸°
4. ì£¼ìš” ê²°ë¡  ê²€ì¦í•˜ê¸°

ê²€ì¦ í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const verifyPrompt: Message = {
    id: 'verify-prompt',
    role: 'user',
    content: `í†µí•©ëœ ì´í•´:\n${integration}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nì´ ì´í•´ë¥¼ ê²€ì¦í•˜ì„¸ìš”:`,
    created_at: Date.now(),
  };

  let verification = '';
  for await (const chunk of LLMService.streamChat([systemMessage2, verifyPrompt])) {
    verification += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  console.log('[Deep] Verification complete');

  return {
    integration,
    verification,
  };
}

/**
 * 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
 */
async function finalSynthesisNode(state: DeepThinkingState) {
  console.log('[Deep] Step 5/5: Generating final comprehensive answer...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## âœ¨ 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ (5/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ëª¨ë“  ì‚¬ê³  ê³¼ì •ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  const systemMessage: Message = {
    id: 'system-final',
    role: 'system',
    content: `${createBaseSystemMessage()}\n\në‹¹ì‹ ì€ ê´‘ë²”ìœ„í•œ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.
ì´ì œ ì´ ëª¨ë“  ì‹¬ì¸µ ì‚¬ê³ ì˜ ì •ì ì„ ë‚˜íƒ€ë‚´ëŠ” ìµœì¢…ì ì´ê³  í¬ê´„ì ì´ë©° ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ëª…í™•í•˜ê³  í†µì°°ë ¥ ìˆìœ¼ë©° ì§ˆë¬¸ì„ ì² ì €íˆ ë‹¤ë£¨ëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  const allContext = `
# ì´ˆê¸° ë¶„ì„
${state.initialAnalysis}

# íƒìƒ‰ëœ ê´€ì ë“¤
${state.perspectives.map((p) => `## ${p.name}\n${p.content}\n\n### ì‹¬í™” ë¶„ì„\n${p.deepAnalysis}`).join('\n\n')}

# í†µí•©
${state.integration}

# ê²€ì¦
${state.verification}
`;

  const finalPrompt: Message = {
    id: 'final-prompt',
    role: 'user',
    content: `ê´‘ë²”ìœ„í•œ ë¶„ì„ í›„, ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ëª¨ë“  ì‚¬ê³  ë‚´ìš©ì…ë‹ˆë‹¤:\n\n${allContext}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nìµœì¢…ì ì´ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:`,
    created_at: Date.now(),
  };

  let finalAnswer = '';
  const messageId = `msg-${Date.now()}`;

  for await (const chunk of LLMService.streamChat([systemMessage, finalPrompt])) {
    finalAnswer += chunk;
    // Send each chunk to renderer via callback for real-time streaming (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  const assistantMessage: Message = {
    id: messageId,
    role: 'assistant',
    content: finalAnswer,
    created_at: Date.now(),
  };

  console.log('[Deep] Final comprehensive answer generated');

  return {
    messages: [assistantMessage],
  };
}

/**
 * Deep Thinking Graph ìƒì„±
 */
export function createDeepThinkingGraph() {
  const workflow = new StateGraph(DeepThinkingStateAnnotation)
    // ë…¸ë“œ ì¶”ê°€
    .addNode('initial_analysis', initialAnalysisNode)
    .addNode('explore_perspectives', explorePerspectivesNode)
    .addNode('deep_analysis', deepAnalysisNode)
    .addNode('integrate_verify', integrateAndVerifyNode)
    .addNode('final_synthesis', finalSynthesisNode)
    // ìˆœì°¨ì  ì—£ì§€
    .addEdge('__start__', 'initial_analysis')
    .addEdge('initial_analysis', 'explore_perspectives')
    .addEdge('explore_perspectives', 'deep_analysis')
    .addEdge('deep_analysis', 'integrate_verify')
    .addEdge('integrate_verify', 'final_synthesis')
    .addEdge('final_synthesis', END);

  return workflow.compile();
}
