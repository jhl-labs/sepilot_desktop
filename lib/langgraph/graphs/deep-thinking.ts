import { StateGraph, END } from '@langchain/langgraph';
import { Annotation } from '@langchain/langgraph';
import { Message } from '@/types';
import { LLMService } from '@/lib/llm/service';
import { createBaseSystemMessage } from '../utils/system-message';
import { emitStreamingChunk, getCurrentGraphConfig } from '@/lib/llm/streaming-callback';
import { generateWithToolsNode } from '../nodes/generate';
import { toolsNode } from '../nodes/tools';

import { logger } from '@/lib/utils/logger';
import type { SupportedLanguage } from '@/lib/i18n';
/**
 * Deep Thinking Graph
 *
 * Sequential Thinkingê³¼ Tree of Thoughtë¥¼ ê²°í•©í•œ ê°€ì¥ ê¹Šì€ ì‚¬ê³  ë°©ì‹
 *
 * í”„ë¡œì„¸ìŠ¤:
 * 1. ì´ˆê¸° ë¶„ì„ (Initial Analysis)
 * 2. ë‹¤ì¤‘ ê´€ì  íƒìƒ‰ (Multi-Perspective Exploration)
 * 3. ê° ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ (Deep Analysis per Perspective)
 * 4. í†µí•© ë° ê²€ì¦ (Integration & Verification)
 * 5. ìµœì¢… ë‹µë³€ ìƒì„± (Final Synthesis)
 */

/**
 * ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
 */
async function getUserLanguage(): Promise<SupportedLanguage> {
  try {
    // Main Processì—ì„œë§Œ ë™ì‘
    if (typeof window !== 'undefined') {
      // Renderer í”„ë¡œì„¸ìŠ¤ì—ì„œëŠ” localStorageì—ì„œ ê°€ì ¸ì˜¤ê¸°
      try {
        const saved = localStorage.getItem('sepilot_language');
        if (saved && ['ko', 'en', 'zh'].includes(saved)) {
          return saved as SupportedLanguage;
        }
      } catch {
        // localStorage ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
      }
      return 'ko';
    }

    const { databaseService } = await import('../../../electron/services/database');
    const configStr = databaseService.getSetting('app_config');
    if (!configStr) {
      return 'ko';
    }

    const appConfig = JSON.parse(configStr);
    if (appConfig?.general?.language && ['ko', 'en', 'zh'].includes(appConfig.general.language)) {
      return appConfig.general.language as SupportedLanguage;
    }
  } catch (error) {
    logger.error('[Deep] Failed to get user language:', error);
  }
  return 'ko';
}

/**
 * ì–¸ì–´ì— ë”°ë¥¸ ë‹µë³€ ì–¸ì–´ ì§€ì‹œ ë©”ì‹œì§€ ìƒì„±
 */
function getLanguageInstruction(language: SupportedLanguage): string {
  switch (language) {
    case 'ko':
      return 'ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.';
    case 'en':
      return 'Please respond in English.';
    case 'zh':
      return 'è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚';
    default:
      return 'ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.';
  }
}

/**
 * RAG ê²€ìƒ‰ í—¬í¼ í•¨ìˆ˜
 */
async function retrieveContextIfEnabled(query: string): Promise<string> {
  const config = getCurrentGraphConfig();
  if (!config?.enableRAG) {
    return '';
  }

  try {
    // Main Process ì „ìš© ë¡œì§
    if (typeof window !== 'undefined') {
      return '';
    }

    logger.info('[Deep] RAG enabled, retrieving documents...');
    const { vectorDBService } = await import('../../../electron/services/vectordb');
    const { databaseService } = await import('../../../electron/services/database');
    const { initializeEmbedding, getEmbeddingProvider } =
      await import('@/lib/vectordb/embeddings/client');

    const configStr = databaseService.getSetting('app_config');
    if (!configStr) {
      return '';
    }
    const appConfig = JSON.parse(configStr);
    if (!appConfig.embedding) {
      return '';
    }

    initializeEmbedding(appConfig.embedding);
    const embedder = getEmbeddingProvider();
    const queryEmbedding = await embedder.embed(query);
    const results = await vectorDBService.searchByVector(queryEmbedding, 5);

    if (results.length > 0) {
      logger.info(`[Deep] Found ${results.length} documents`);
      return results.map((doc, i) => `[ì°¸ê³  ë¬¸ì„œ ${i + 1}]\n${doc.content}`).join('\n\n');
    }
  } catch (error) {
    console.error('[Deep] RAG retrieval failed:', error);
  }
  return '';
}

/**
 * 0ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ (Research)
 */
async function researchNode(state: DeepThinkingState) {
  logger.info('[Deep] Step 0: Researching...');
  emitStreamingChunk('\n\n## ğŸ” 0ë‹¨ê³„: ì •ë³´ ìˆ˜ì§‘ (Research)\n\n', state.conversationId);

  // RAG ê²€ìƒ‰
  const query = state.messages[state.messages.length - 1].content;
  const ragContext = await retrieveContextIfEnabled(query);

  let gatheredInfo = ragContext ? `[RAG ê²€ìƒ‰ ê²°ê³¼]\n${ragContext}\n\n` : '';

  // ë„êµ¬ ì‚¬ìš© ë£¨í”„ (ìµœëŒ€ 5íšŒ)
  let currentMessages = [...state.messages];

  // ì‹œìŠ¤í…œ ë©”ì‹œì§€: ì •ë³´ ìˆ˜ì§‘ê°€ í˜ë¥´ì†Œë‚˜
  const researchSystemMsg: Message = {
    id: 'system-research',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì‹¬ì¸µ ë¶„ì„ì„ í•˜ê¸° ì „, í•„ìš”í•œ ë°°ê²½ ì§€ì‹ê³¼ ìµœì‹  ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ì—°êµ¬ì›ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë„êµ¬(ê²€ìƒ‰ ë“±)ë¥¼ í™œìš©í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.
ì´ë¯¸ ì¶©ë¶„í•œ ì •ë³´ê°€ ìˆê±°ë‚˜ ë„êµ¬ê°€ ì—†ë‹¤ë©´ ì¦‰ì‹œ ì¢…ë£Œí•˜ì„¸ìš”.
ìµœëŒ€ 3íšŒì˜ ê¸°íšŒê°€ ìˆìŠµë‹ˆë‹¤.`,
    created_at: Date.now(),
  };

  currentMessages = [researchSystemMsg, ...currentMessages];

  for (let i = 0; i < 3; i++) {
    // Generate (ë„êµ¬ ì‚¬ìš© ê²°ì •)
    const genResult = await generateWithToolsNode({
      ...state,
      messages: currentMessages,
      context: '',
      toolCalls: [],
      toolResults: [],
      generatedImages: [],
      planningNotes: {},
    });
    const responseMsg = genResult.messages?.[0];

    if (!responseMsg) {
      break;
    }

    currentMessages.push(responseMsg);

    if (!responseMsg.tool_calls || responseMsg.tool_calls.length === 0) {
      break;
    }

    // Tools Execute
    const toolNames = responseMsg.tool_calls.map((tc) => tc.name).join(', ');
    emitStreamingChunk(`\nğŸ› ï¸ **ì •ë³´ ìˆ˜ì§‘ ì¤‘:** ${toolNames}...\n`, state.conversationId);

    const toolResult = await toolsNode({
      ...state,
      messages: currentMessages,
      planningNotes: {},
      context: '',
      toolCalls: [],
      generatedImages: [],
      toolResults: [],
    });

    // ê²°ê³¼ ë©”ì‹œì§€ ìƒì„±
    const toolMessages = (toolResult.toolResults || []).map((res) => ({
      role: 'tool' as const,
      tool_call_id: res.toolCallId,
      name: res.toolName,
      content: res.result || res.error || '',
      id: `tool-${res.toolCallId}`,
      created_at: Date.now(),
    }));

    currentMessages.push(...toolMessages);

    // ìˆ˜ì§‘ëœ ì •ë³´ ëˆ„ì 
    gatheredInfo += `[ë„êµ¬ ì‹¤í–‰ ê²°ê³¼: ${toolNames}]\n${toolMessages.map((m) => m.content).join('\n')}\n\n`;

    emitStreamingChunk(`âœ… **ìˆ˜ì§‘ ì™„ë£Œ**\n`, state.conversationId);
  }

  logger.info('[Deep] Research complete');

  return {
    researchContext: gatheredInfo,
  };
}

export const DeepThinkingStateAnnotation = Annotation.Root({
  messages: Annotation<Message[]>({
    reducer: (existing: Message[], updates: Message[]) => [...existing, ...updates],
    default: () => [],
  }),
  initialAnalysis: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  researchContext: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  perspectives: Annotation<
    Array<{ id: string; name: string; content: string; deepAnalysis: string }>
  >({
    reducer: (_existing: any[], updates: any[]) => updates,
    default: () => [],
  }),
  integration: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  verification: Annotation<string>({
    reducer: (_existing: string, update: string) => update,
    default: () => '',
  }),
  // conversationId: ë™ì‹œ ëŒ€í™” ì‹œ ìŠ¤íŠ¸ë¦¬ë° ê²©ë¦¬ë¥¼ ìœ„í•´ í•„ìˆ˜
  conversationId: Annotation<string>({
    reducer: (_existing: string, update: string) => update || _existing,
    default: () => '',
  }),
});

export type DeepThinkingState = typeof DeepThinkingStateAnnotation.State;

/**
 * 1ë‹¨ê³„: ì´ˆê¸° ë¶„ì„
 */
async function initialAnalysisNode(state: DeepThinkingState) {
  logger.info('[Deep] Step 1/5: Initial comprehensive analysis...');

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n## ğŸ§  1ë‹¨ê³„: ì´ˆê¸° ì‹¬ì¸µ ë¶„ì„ (1/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ë¬¸ì œì— ëŒ€í•œ í¬ê´„ì ì¸ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  // ìˆ˜ì§‘ëœ ì •ë³´(Research/RAG) ê°€ì ¸ì˜¤ê¸°
  const query = state.messages[state.messages.length - 1].content;
  const researchContext = state.researchContext;

  if (researchContext) {
    emitStreamingChunk(`\nğŸ“š **ì‚¬ì „ ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.**\n\n`, state.conversationId);
  }

  const systemMessage: Message = {
    id: 'system',
    role: 'system',
    content: `ë‹¹ì‹ ì€ í¬ê´„ì ì¸ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê³ ë„ë¡œ ë¶„ì„ì ì¸ AIì…ë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. í•µì‹¬ ì§ˆë¬¸ì„ ê¹Šì´ ì´í•´í•˜ê¸°
2. ëª¨ë“  ê´€ë ¨ ì¸¡ë©´ê³¼ ì°¨ì› íŒŒì•…í•˜ê¸°
3. ì´ ì§ˆë¬¸ì´ ë³µì¡í•˜ê±°ë‚˜ ë¯¸ë¬˜í•œ ì´ìœ  ê³ ë ¤í•˜ê¸°
4. íƒìƒ‰í•  ê°€ì¹˜ê°€ ìˆëŠ” ê´€ì  ê²°ì •í•˜ê¸°

ì² ì €í•˜ê³  ìƒì„¸í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const analysisPrompt: Message = {
    id: 'analysis-prompt',
    role: 'user',
    content: `ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ í¬ê´„ì ì¸ ì´ˆê¸° ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:\n\n${query}\n\n${researchContext ? `ìˆ˜ì§‘ëœ ì •ë³´:\n${researchContext}\n\n` : ''}ìœ„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë¶„ì„í•˜ì„¸ìš”.`,
    created_at: Date.now(),
  };

  let analysis = '';
  for await (const chunk of LLMService.streamChat(
    [systemMessage, ...state.messages, analysisPrompt],
    { tools: [], tool_choice: 'none' }
  )) {
    analysis += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  logger.info('[Deep] Initial analysis complete');

  return {
    initialAnalysis: analysis,
  };
}

/**
 * 2ë‹¨ê³„: ë‹¤ì¤‘ ê´€ì  íƒìƒ‰
 */
async function explorePerspectivesNode(state: DeepThinkingState) {
  logger.info('[Deep] Step 2/5: Exploring multiple perspectives...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸ”­ 2ë‹¨ê³„: ë‹¤ì¤‘ ê´€ì  íƒìƒ‰ (2/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë¬¸ì œ í•´ê²° ë°©ë²•ì„ íƒìƒ‰ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  const perspectiveTypes = [
    { name: 'ë¶„ì„ì  ê´€ì ', focus: 'ë…¼ë¦¬ì  ì¶”ë¡ , ì‚¬ì‹¤, ë°ì´í„°, ì²´ê³„ì  ë¶„ì„' },
    { name: 'ì‹¤ìš©ì  ê´€ì ', focus: 'ì‹¤ì œ ì ìš©, ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸, ì‹¤ìš©ì  í•´ê²°ì±…' },
    { name: 'ë¹„íŒì  ê´€ì ', focus: 'ì ì¬ì  ë¬¸ì œ, í•œê³„, ë°˜ë¡ , ì—£ì§€ ì¼€ì´ìŠ¤' },
    { name: 'ì°½ì˜ì  ê´€ì ', focus: 'í˜ì‹ ì  ì•„ì´ë””ì–´, ëŒ€ì•ˆì  ì ‘ê·¼, ë¹„ì „í†µì  ì‚¬ê³ ' },
  ];

  const perspectives: Array<{ id: string; name: string; content: string; deepAnalysis: string }> =
    [];

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  for (const type of perspectiveTypes) {
    // ê° ê´€ì  ì‹œì‘ ì•Œë¦¼
    emitStreamingChunk(`\n### ğŸ‘ï¸ ${type.name}\n\n`, state.conversationId);

    const systemMessage: Message = {
      id: `system-${type.name}`,
      role: 'system',
      content: `ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ëŒ€í•´ ${type.name} ê´€ì ì„ íƒìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì§‘ì¤‘ ì˜ì—­: ${type.focus}

ì´ˆê¸° ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì´ íŠ¹ì • ê´€ì ì—ì„œ í†µì°°ì„ ì œê³µí•˜ì„¸ìš”. ${languageInstruction}`,
      created_at: Date.now(),
    };

    const perspectivePrompt: Message = {
      id: `perspective-${type.name}`,
      role: 'user',
      content: `ì´ˆê¸° ë¶„ì„:\n${state.initialAnalysis}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\n${type.name}ì—ì„œ ì´ ì§ˆë¬¸ì„ íƒìƒ‰í•˜ì„¸ìš”:`,
      created_at: Date.now(),
    };

    let content = '';
    for await (const chunk of LLMService.streamChat([systemMessage, perspectivePrompt], {
      tools: [],
      tool_choice: 'none',
    })) {
      content += chunk;
      // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
      emitStreamingChunk(chunk, state.conversationId);
    }

    logger.info(`[Deep] ${type.name} perspective explored`);

    perspectives.push({
      id: type.name.toLowerCase(),
      name: type.name,
      content,
      deepAnalysis: '', // Will be filled in next step
    });
  }

  return {
    perspectives,
  };
}

/**
 * 3ë‹¨ê³„: ê° ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„
 */
async function deepAnalysisNode(state: DeepThinkingState) {
  logger.info('[Deep] Step 3/5: Performing deep analysis on each perspective...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸ”¬ 3ë‹¨ê³„: ê´€ì ë³„ ì‹¬í™” ë¶„ì„ (3/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ê° ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ì„ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  const deepAnalyzedPerspectives: Array<{
    id: string;
    name: string;
    content: string;
    deepAnalysis: string;
  }> = [];

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  for (const perspective of state.perspectives) {
    // ê° ì‹¬í™” ë¶„ì„ ì‹œì‘ ì•Œë¦¼
    emitStreamingChunk(`\n### ğŸ” ${perspective.name} ì‹¬í™” ë¶„ì„\n\n`, state.conversationId);

    const systemMessage: Message = {
      id: `system-deep-${perspective.id}`,
      role: 'system',
      content: `ë‹¹ì‹ ì€ ${perspective.name}ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. í•µì‹¬ í¬ì¸íŠ¸ë¥¼ ë” ê¹Šì´ ê²€í† í•˜ê¸°
2. í•¨ì˜ì™€ ê²°ê³¼ ê³ ë ¤í•˜ê¸°
3. ì•„ì´ë””ì–´ ì—°ê²° ë° íŒ¨í„´ íŒŒì•…í•˜ê¸°
4. ë…¼ì¦ì„ ì¶”ë¡ ìœ¼ë¡œ ê°•í™”í•˜ê¸°

ìƒì„¸í•˜ê³  ì‹¬ì¸µì ì¸ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”. ${languageInstruction}`,
      created_at: Date.now(),
    };

    const deepAnalysisPrompt: Message = {
      id: `deep-analysis-${perspective.id}`,
      role: 'user',
      content: `${perspective.name}:\n${perspective.content}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nì´ ê´€ì ì— ëŒ€í•œ ì‹¬í™” ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:`,
      created_at: Date.now(),
    };

    let deepAnalysis = '';
    for await (const chunk of LLMService.streamChat([systemMessage, deepAnalysisPrompt], {
      tools: [],
      tool_choice: 'none',
    })) {
      deepAnalysis += chunk;
      // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
      emitStreamingChunk(chunk, state.conversationId);
    }

    logger.info(`[Deep] ${perspective.name} perspective deeply analyzed`);

    deepAnalyzedPerspectives.push({
      ...perspective,
      deepAnalysis,
    });
  }

  return {
    perspectives: deepAnalyzedPerspectives,
  };
}

/**
 * 4ë‹¨ê³„: í†µí•© ë° ê²€ì¦
 */
async function integrateAndVerifyNode(state: DeepThinkingState) {
  logger.info('[Deep] Step 4/5: Integrating perspectives and verifying...');

  // í†µí•© ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## ğŸ”— 4ë‹¨ê³„: í†µí•© ë° ê²€ì¦ (4/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ê´€ì ë“¤ì„ í†µí•©í•˜ê³  ê²°ê³¼ì˜ ìœ íš¨ì„±ì„ ê²€ì¦ ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );
  emitStreamingChunk('### ğŸ“¦ ê´€ì  í†µí•©\n\n', state.conversationId);

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  const systemMessage1: Message = {
    id: 'system-integrate',
    role: 'system',
    content: `ë‹¹ì‹ ì€ ì—¬ëŸ¬ ê´€ì ì˜ í†µì°°ì„ ì¼ê´€ëœ ì´í•´ë¡œ í†µí•©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. ê´€ì ë“¤ ê°„ì˜ ê³µí†µ ì£¼ì œ íŒŒì•…í•˜ê¸°
2. ëª¨ìˆœ í•´ê²°í•˜ê¸°
3. ìƒí˜¸ë³´ì™„ì  í†µì°° ì¢…í•©í•˜ê¸°
4. í¬ê´„ì  ì´í•´ êµ¬ì¶•í•˜ê¸°

ì² ì €í•˜ê³  ì„¬ì„¸í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const allPerspectives = state.perspectives
    .map((p) => `### ${p.name}:\n${p.content}\n\n#### Deep Analysis:\n${p.deepAnalysis}`)
    .join('\n\n---\n\n');

  const integratePrompt: Message = {
    id: 'integrate-prompt',
    role: 'user',
    content: `ì´ˆê¸° ë¶„ì„:\n${state.initialAnalysis}\n\nëª¨ë“  ê´€ì :\n${allPerspectives}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nëª¨ë“  í†µì°°ì„ ì¼ê´€ëœ ì´í•´ë¡œ í†µí•©í•˜ì„¸ìš”:`,
    created_at: Date.now(),
  };

  let integration = '';
  for await (const chunk of LLMService.streamChat([systemMessage1, integratePrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    integration += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  logger.info('[Deep] Integration complete, now verifying...');

  // ê²€ì¦ ë‹¨ê³„ ì•Œë¦¼
  emitStreamingChunk('\n\n### âœ… ê²€ì¦ ë‹¨ê³„\n\n', state.conversationId);

  const systemMessage2: Message = {
    id: 'system-verify',
    role: 'system',
    content: `ë‹¹ì‹ ì€ í†µí•©ëœ ì´í•´ì˜ ì™„ì „ì„±ê³¼ ì •í™•ì„±ì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤.

ë‹¹ì‹ ì˜ ê³¼ì œ:
1. ì§ˆë¬¸ì˜ ëª¨ë“  ì¸¡ë©´ì´ ë‹¤ë¤„ì¡ŒëŠ”ì§€ í™•ì¸í•˜ê¸°
2. ë¹ˆí‹ˆì´ë‚˜ ì•½ì  íŒŒì•…í•˜ê¸°
3. ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ë³´í•˜ê¸°
4. ì£¼ìš” ê²°ë¡  ê²€ì¦í•˜ê¸°

ê²€ì¦ í‰ê°€ë¥¼ ì œê³µí•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const verifyPrompt: Message = {
    id: 'verify-prompt',
    role: 'user',
    content: `í†µí•©ëœ ì´í•´:\n${integration}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nì´ ì´í•´ë¥¼ ê²€ì¦í•˜ì„¸ìš”:`,
    created_at: Date.now(),
  };

  let verification = '';
  for await (const chunk of LLMService.streamChat([systemMessage2, verifyPrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    verification += chunk;
    // ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  logger.info('[Deep] Verification complete');

  return {
    integration,
    verification,
  };
}

/**
 * 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ ìƒì„±
 */
async function finalSynthesisNode(state: DeepThinkingState) {
  logger.info('[Deep] Step 5/5: Generating final comprehensive answer...');

  // ë‹¨ê³„ ì‹œì‘ ì•Œë¦¼
  emitStreamingChunk('\n\n---\n\n## âœ¨ 5ë‹¨ê³„: ìµœì¢… ë‹µë³€ (5/5)\n\n', state.conversationId);
  emitStreamingChunk(
    '**ë‹¨ê³„ ì§„í–‰ ì¤‘:** ëª¨ë“  ì‚¬ê³  ê³¼ì •ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n\n',
    state.conversationId
  );

  // ì‚¬ìš©ì ì–¸ì–´ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
  const userLanguage = await getUserLanguage();
  const languageInstruction = getLanguageInstruction(userLanguage);

  const systemMessage: Message = {
    id: 'system-final',
    role: 'system',
    content: `${createBaseSystemMessage()}\n\në‹¹ì‹ ì€ ê´‘ë²”ìœ„í•œ ì‚¬ê³  ê³¼ì •ì„ ê±°ì³¤ìŠµë‹ˆë‹¤.
ì´ì œ ì´ ëª¨ë“  ì‹¬ì¸µ ì‚¬ê³ ì˜ ì •ì ì„ ë‚˜íƒ€ë‚´ëŠ” ìµœì¢…ì ì´ê³  í¬ê´„ì ì´ë©° ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ëª…í™•í•˜ê³  í†µì°°ë ¥ ìˆìœ¼ë©° ì§ˆë¬¸ì„ ì² ì €íˆ ë‹¤ë£¨ëŠ” ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”. ${languageInstruction}`,
    created_at: Date.now(),
  };

  const allContext = `
# ì´ˆê¸° ë¶„ì„
${state.initialAnalysis}

# íƒìƒ‰ëœ ê´€ì ë“¤
${state.perspectives.map((p) => `## ${p.name}\n${p.content}\n\n### ì‹¬í™” ë¶„ì„\n${p.deepAnalysis}`).join('\n\n')}

# í†µí•©
${state.integration}

# ê²€ì¦
${state.verification}
`;

  const finalPrompt: Message = {
    id: 'final-prompt',
    role: 'user',
    content: `ê´‘ë²”ìœ„í•œ ë¶„ì„ í›„, ì§€ê¸ˆê¹Œì§€ ìˆ˜í–‰ëœ ëª¨ë“  ì‚¬ê³  ë‚´ìš©ì…ë‹ˆë‹¤:\n\n${allContext}\n\nì›ë³¸ ì§ˆë¬¸: ${state.messages[state.messages.length - 1].content}\n\nìµœì¢…ì ì´ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”:`,
    created_at: Date.now(),
  };

  let finalAnswer = '';
  const messageId = `msg-${Date.now()}`;

  for await (const chunk of LLMService.streamChat([systemMessage, finalPrompt], {
    tools: [],
    tool_choice: 'none',
  })) {
    finalAnswer += chunk;
    // Send each chunk to renderer via callback for real-time streaming (conversationIdë¡œ ê²©ë¦¬)
    emitStreamingChunk(chunk, state.conversationId);
  }

  const assistantMessage: Message = {
    id: messageId,
    role: 'assistant',
    content:
      `## ğŸ§  1ë‹¨ê³„: ì´ˆê¸° ì‹¬ì¸µ ë¶„ì„\n\n${state.initialAnalysis}\n\n` +
      `## ğŸ”­ 2ë‹¨ê³„ & 3ë‹¨ê³„: ë‹¤ì¤‘ ê´€ì  íƒìƒ‰ ë° ë¶„ì„\n\n${state.perspectives
        .map((p) => `### ğŸ‘ï¸ ${p.name}\n${p.content}\n\n#### ğŸ” ì‹¬í™” ë¶„ì„\n${p.deepAnalysis}`)
        .join('\n\n')}\n\n` +
      `## ğŸ”— 4ë‹¨ê³„: í†µí•© ë° ê²€ì¦\n\n### ğŸ“¦ ê´€ì  í†µí•©\n${state.integration}\n\n### âœ… ê²€ì¦\n${state.verification}\n\n` +
      `---\n\n## âœ¨ ìµœì¢… ë‹µë³€\n\n${finalAnswer}`,
    created_at: Date.now(),
  };

  logger.info('[Deep] Final comprehensive answer generated');

  return {
    messages: [assistantMessage],
  };
}

/**
 * Deep Thinking Graph ìƒì„±
 */
export function createDeepThinkingGraph() {
  const workflow = new StateGraph(DeepThinkingStateAnnotation)
    // ë…¸ë“œ ì¶”ê°€
    .addNode('research', researchNode)
    .addNode('initial_analysis', initialAnalysisNode)
    .addNode('explore_perspectives', explorePerspectivesNode)
    .addNode('deep_analysis', deepAnalysisNode)
    .addNode('integrate_verify', integrateAndVerifyNode)
    .addNode('final_synthesis', finalSynthesisNode)
    // ìˆœì°¨ì  ì—£ì§€
    .addEdge('__start__', 'research')
    .addEdge('research', 'initial_analysis')
    .addEdge('initial_analysis', 'explore_perspectives')
    .addEdge('explore_perspectives', 'deep_analysis')
    .addEdge('deep_analysis', 'integrate_verify')
    .addEdge('integrate_verify', 'final_synthesis')
    .addEdge('final_synthesis', END);

  return workflow.compile();
}
